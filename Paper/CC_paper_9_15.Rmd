---
title: "Emoji"
author: "Tek"
header-includes:
   - \usepackage{amsmath}
   - \usepackage{graphicx}
   - \usepackage{bbm}
   - \usepackage{subcaption}
   - \usepackage[export]{adjustbox}
   - \usepackage{wrapfig}
   - \usepackage{color}
   - \usepackage{enumerate}
output:
  pdf_document:
    fig_caption: yes
    number_sections: no
geometry: margin=0.5in
---
```{r hidecode, echo=FALSE}
library(knitr)
opts_chunk$set(echo=F, warning=FALSE, message = FALSE, tidy.opts=list(width.cutoff=40))
```

```{r library and source, include=F}
Sys.setenv(JAVA_HOME='C:/Program Files/Java/jre1.8.0_121')
pkg=c("tidytext","tidyverse", "gsubfn", "Unicode","pander","openNLP", "NLP", "topicmodels", "tm", "RTextTools", "gridExtra")
sapply(pkg, require, character=T)
'%!in%' <- function(x,y)!('%in%'(x,y))
```

# Introduction

<!--intelligence, customer care service, social media analysis, contextual advertising, and more.-->
The development of Social Network Service(SNS) and User Generated Content(UGC) platforms have been providing a new channels for its users to communicate and share opinions with the fellow community members. Hundreds and thousands of new messages are shared every day on Twitter and Facebook; product reviews are posted on Amazon and Trip Advisor. 
The text contents of SNS and UGC is a raw information of Individual's perception and emotion. As one would expect, this distinct characteristic of SNS and UGC text data could be a valuable information in different academic disciplines and industries. As a result, different machine learning and natural language processing techniques were developed to evaluate text data.

Most natural language processing(NLP) techniques require preparation procedures to improve its performance. Deleting stop words(determiners, conjunctions, prepositions, pronouns, auxiliary verbs, modals, and quantifiers) is an example of this preceding step. The result of this step increases the accuracy of the NLP by reducing the noise generated from words that does not convey any contextual meaning. In this context, Emoji(a pictographic information that carries class of feelings) in the text data has been considered as a noise and was deleted prior to applying NLP techniques. 

Although deleting Emoji Unicode before NLP is a standard operation, unlike deleting stop words, this does take away information that might have modest contribution to the context. Emoji, originally driven from Japanese word e(picture) + moji(character), is a pictograph that has become widely used on internet web pages and on SNS plaforms. Emoji, much like its close relative emoticon, could provide visual representation of not only solid objects, but also emotions through facial expressions and symbols related to feelings and moods. Communication via traditional text characters, such as words in alphabets, may not be as effective and efficient as Emojis when conveying emotions. For this reason, Emoji gained popularity after 1990 especially after cell phones and internet came to wide use. Therefore, simply deleting Emoji would reduce the information contained in the original text data.

In contrast to filtering Emoji characters out, reflecting Emoji information during the evaluation have benefits. First, eacho Emoji has pre-determined topic dimension set by the official organization. Therefore, crude topic matching may be accomplished using the Emoji information. Second, Emoji may be helpful for sentiment analysis. Being closely related to emotions and mood, sentiment analysis on Emoji will provide auxiliary information.

The `emoji` package in `R` was written to help the above analysis.


# Emoji Data Set

## `emoji` package in R

\textbf{Plan to change this part after posting the Emoji package on CRAN}

Emoji in a text data is encoded as a sequence of Unicode: an industrial standard that consists encoding, representation, and text expression of writing system. \textit{The Unicode Standard} is distributed by a non-profit organization the \textit{Unicode Consortium}. The current list of Emoji v5.0 is available on the [official \textit{Unicode Consortium} website](http://unicode.org/emoji/charts/full-emoji-list.html). Example illustration of the Emoji table is attached in \autoref{fig:uni_web}.  Data set of Emoji characters are available in `emoji` package.

\begin{figure}[htbp]
\centering
\includegraphics[width=100mm]{"unicode_webpage".png}
\caption{Glimpse of the table of Emoji on the unicode.org website \label{fig:uni_web}}
\end{figure}

## Generate different type of encodings using Python

\textbf{A script was written R that changes between different encoding environment. Plan to include this feature in the `emoji` package.}\\
There are multiple way of encoding Emojis on website or SNS. Unicode, Unicode escape, UTF-8hex, zerox notation, NCR are examples of commonly used encodings. As shown in \autoref{fig:uni_web}, the initial data set scraped from the \textit{Unicode Consortium} only has Unicode. The most common encoding type for online web page, however, is UTF-8 and Unicode escape. Therefore, the original Unicode sequence should be translated into different encoding types for the data set to be applied. Different types of encoding format were generated from the original Unicode via simple Python code.

## Scoring

The characteristic of Emoji (effectively delivers feelings and moods), naturally leads text mining with Emoji to sentiment analysis. `tidytext` package in R has three general purpose lexicon sets. The `AFINN` score words from -5 to 5 scale, `bing` assigns words in binary category(positive and negative), and `nrc`assigns words with more categories. 





# Description of the Emoji Data set

\textbf{The table should be updated.  It is using the v4.0 Emoji list}

The complete Emoji data set is saved under the 'data' directory. This complete data set is read and named 'uni_info'. Some Emojis are a combination of two or more basic Emojis. For example, Emoji 'boy: light skin tone' is a combination of 'boy' (U+1F466) and 'light skin tone' (U+1F3FB). Data 'basic_uni_info' is a data set of the basic Emojis. There are many different ways to encode 'Unicode'. The data set includes the following encoding types: 'U+hexadecimal', 'UTF-8 hexadecimal', 'hexadecimal', and 'numeric character reference (NCR)'. The example of the data set is given in \autoref{tab:dataset}

```{r show_dataset, echo=F}
uni_info=readRDS("../data/uni_info_v8.Rds")
egdat=head(uni_info,5)
edgat=egdat[,-c(6:7,13)]

pander(edgat, split.table = 180, style = 'rmarkdown', caption = "\\label{tab:dataset}Information of 5 Emoji data set")
```

\pagebreak

# Application

###Data Set and exploratory data analysis

Two samples of twitter messages with the following hastag #inlove and #hateher were scraped. The data set contains 944 #inlove messages, 1145 #hateher messages, and 1195 #marchscience messages. The proportion of Twitter messages containing Emoji characters per hashtag is illustraited in \autoref{tab:EProp}. 52.7% of the #inlove message strings, 29.3% of the #hateher message strings, and 7.8% of #marchscience message strings have one or more emoji information.

```{r load_twitter_data, eval=T, echo=F}
twitter_example_inlove=readRDS("../data/twitter_inlove.Rds")
twitter_example_hateher=readRDS("../data/twitter_hateher.Rds")
twitter_example_marchscience=readRDS("../data/twitter_marchscience.Rds")
```

```{r, echo=F}
prop_emoji=cbind.data.frame(498/944, 335/1145, 93/1195)
colnames(prop_emoji)<-c("#inlove", "#hateher", "#marchscience")
rownames(prop_emoji)<-"Proportion"

pander(prop_emoji, split.table = 180, style = 'rmarkdown', caption = "\\label{tab:EProp} Proportion of Twitter messages with Emoji")
```

```{r, eval=F, echo=F}
filter(twitter_example_inlove, grepl("U\\+",V1)) %>%
  unnest_tokens(word, V1, token = "regex", pattern = "[\\s]") %>%
  select(word) %>%
  filter(grepl("u\\+",word)) %>%
  count(word, sort = TRUE) %>%
  head(5)->inlove_uni_count
colnames(inlove_uni_count)<-c("#inlove","Count")

filter(twitter_example_hateher, grepl("U\\+",V1)) %>%
  unnest_tokens(word, V1, token = "regex", pattern = "[\\s]") %>%
  select(word) %>%
  filter(grepl("u\\+",word)) %>%
  count(word, sort = TRUE) %>%
  head(5)->hateher_uni_count
colnames(hateher_uni_count)<-c("#hateher","Count")

filter(twitter_example_marchscience, grepl("U\\+",V1)) %>%
  unnest_tokens(word, V1, token = "regex", pattern = "[\\s]") %>%
  select(word) %>%
  filter(grepl("u\\+",word)) %>%
  count(word, sort = TRUE) %>%
  head(5)->marchscience_uni_count
colnames(marchscience_uni_count)<-c("#marchscience","Count")

emoji_freq=cbind.data.frame(inlove_uni_count, hateher_uni_count, marchscience_uni_count)

pander(emoji_freq, split.table = 180, style = 'rmarkdown', caption = "\\label{tab:EPopular} Five most popular Emoji for each hastags")

# xdate_number=xtable(emoji_freq, caption = "Five most popular Emoji for each hastags", label = "tab:EPopular")
# print(xdate_number)
```

For hashtag #inlove, total number of 1188 Emojis were used from 182 unique emojis. 
For hashtag #hateher, 695 Emojis from 112 unique Emojis were used.
For hashtag #sciencemarch, 202 Emojis from 102 unique Emojis were used (Note that there may be multiple Emojis per Twitter message).
Top 5 frequently used Emojis per hashtag is given in \autoref{tab:EPopular}.

\begin{table}[htbp]
\centering
\begin{tabular}{ccccccccc}
  \hline
  \#inlove & Emoji & Count & \#hateher & Emoji & Count & \#marchscience & Emoji & Count \\ 
  \hline
U+1F60D & \includegraphics[width=0.03\textwidth, height=60mm]{images/U+1F60D.png} & 297 & U+1F602 & \includegraphics[width=0.03\textwidth, height=60mm]{images/U+1F602.png} & 154 & U+1F52C &  \includegraphics[width=0.03\textwidth, height=60mm]{images/U+1F52C.png} & 13 \\ 
U+2764 & \includegraphics[width=0.03\textwidth, height=60mm]{images/U+2764.png} & 164 & U+1F644 &  \includegraphics[width=0.03\textwidth, height=60mm]{images/U+1F644.png} & 88 & U+1F30E &  \includegraphics[width=0.03\textwidth, height=60mm]{images/U+1F30E.png} & 11 \\ 
U+1F495 & \includegraphics[width=0.03\textwidth, height=60mm]{images/U+1F495.png} & 47 & U+1F621 &  \includegraphics[width=0.03\textwidth, height=60mm]{images/U+1F621.png} & 40 & U+1F44D &   \includegraphics[width=0.03\textwidth, height=60mm]{images/U+1F44D.png} & 9 \\ 
U+1F618 & \includegraphics[width=0.03\textwidth, height=60mm]{images/U+1F618.png} & 40 & U+1F612 &  \includegraphics[width=0.03\textwidth, height=60mm]{images/U+1F612.png} & 38 & U+1F680 &   \includegraphics[width=0.03\textwidth, height=60mm]{images/U+1F680.png} & 8 \\ 
U+2728 & \includegraphics[width=0.03\textwidth, height=60mm]{images/U+2728.png} & 26 & U+1F62D &  \includegraphics[width=0.03\textwidth, height=60mm]{images/U+1F62D.png} & 36 & U+1F30D &   \includegraphics[width=0.03\textwidth, height=60mm]{images/U+1F30D.png} & 7 \\ 
   \hline
\end{tabular}
\caption{Five most popular Emoji for each hastags} 
\label{tab:EPopular}
\end{table}

<!--
\begin{figure}[htbp]
\centering
\includegraphics[width=150mm]{"pop_emoji".png}
\caption{Five most popular Emoji for each hastags \label{fig:EPopular}}
\end{figure}
-->

It is interesting to see "Face with tear of joy" as the most popular Emoji for hashtag #hateher. Although the name itself contains the word "joy", some users of this Emoji adopted this pictogram to express their mixed feeling of love and hate at the same time.



###LDA
To address the importance or reflecting the emoji information in text data, Latent Dirichlet Allocation (LDA): a popular topic modeling method, was performed on twitter messages scraped online. LDA is a topic modeling method that allows words observed in documents to be explained by unobserved topics and that each word's creation is attributable to one of the document's topics.

LDA is based on the two following principles:

\begin{enumerate}
\item Every document is a mixture of topics
\item Every topic is a mixture of words
\end {enumerate}

To illustrate, a news paper document may contain several topics such as "politics", "economy", "spots", "entertainment", and etc. For a given topic "politics", common words may be "government", "trump", "president", "congress", and etc. 

LDA assumes that the probability of documents are rnadom mixture over unseen topics, and document $i$ having topic $k$ follows a dirichlet distribution with some parameter $\alpha$. That is, if the probability of document $i$ having topic $k$ is denoted as $\theta_{i,k}$, then $\theta_i \sim Dir(\alpha)$. The second assumption says each topic is a mixture of words, and that the distribution of $n^{th}$ word will follow a multinomial distribution conditioned on the topic z. The probability of word given a topic is denoted as $\beta$.  Then $\beta$ has a Dirichlet distribution with parameter $\eta$.

\begin{enumerate}
\item $\theta_i \sim Dir(\alpha), i=1, \dots, M$
\item $\theta_{i,k}$ is the probability that document $i \in \big\{1, \dots, M \big\}$ has topic $k \in \big\{1, \dots, K \big\}$.
\item $z$ is word's topic drawn from a Multinomial distribution with parameter \textbf{$\theta$}, i.e. $z \sim Multi(\theta)$
\item $\beta_k \sim Dir(\eta), k=1, \dots, K$
\item $\beta_{k,v}$ is the probability of word $v \in \big\{1, \dots, V \big\}$ in topic $k \in \big\{1, \dots, K \big\}$
\item $w$ is a word drawn from a Multinomial distribution with parameter Z and $\beta$, i.e., $w \sim Multi(z, \beta)$.
\end{enumerate}

The marginal distribution of word w given hyper parameter $\alpha$ and $\beta$ is obtained by the following equation:

$$p(w|\alpha, \beta)= \int p(\theta|\alpha)(\prod_{v=1}^{V} \sum_{z_{v}} p(z_v|\theta)p(w_v|z_v, \beta))d\theta$$
where 

Graphical display of LDA is given in \autoref{fig:LDA}.

\begin{figure}[htbp]
\centering
\includegraphics[width=100mm]{"LDA_model".png}
\caption{Graphical Model representation of LDA \label{fig:LDA}}
\end{figure}

#LDA Equation goes here


LDA was performed on the following three difference cases:

\begin{enumerate}
\item LDA on a raw data set
\item LDA on a data set with Unicode removed
\item LDA on a data set with Emoji translated to text
\end{enumerate}


```{r application setup, echo=F, eval=T}
StopNStem=function(text){
  vdc=VectorSource(text)
  my.corpus = Corpus(vdc)
  my.corpus_copy = my.corpus
  my.corpus = tm_map(my.corpus, removeWords, c("the", stopwords("english"))) 
  my.corpus = tm_map(my.corpus, stemDocument, language="english")
  return(paste(strwrap(my.corpus[[1]]), sep="", collapse=""))
}

cleaning0=function(string){
  string=gsub("&amp;", "", string)
  string=gsub("#", "", string)
  string=gsub("&lt;", "<", string)   # <3
  string=gsub(" [:./[:alnum:]]+.com[/[:alnum:]]*", " ", string)
  string=gsub(" http[:./[:alnum:]]+", " ", string)
  string=gsub(" {2,}", " ", string)
  return(string)
}

dat=rbind.data.frame(
  readRDS("../Data/twitter_inlove.Rds") %>% 
    mutate(tag = "inlove"),
  readRDS("../Data/twitter_hateher.Rds")%>% 
    mutate(tag = "hateher"),
  readRDS("../data/twitter_marchscience.Rds") %>%
    mutate(tag = "marchscience"), stringsAsFactors = F
  )

dat$V1 %>% tolower() %>%  cleaning0()-> dat$V1
```

## LDA on a raw data set
The second case was to run LDA on a raw data set. Stemming and stop word deletion were performed. Different number of topic dimensions were tested and the result of 4 topic dimension with 10 terms are provided in \autoref{tab:LDAraw}. Describe the output.


```{r LDAraw, echo=F, eval=T}
stop_stem=lapply(dat$V1, StopNStem) %>% 
  do.call("rbind",.) %>% 
  rbind.data.frame(stringsAsFactors=F)

stop_stem %>%
  select(V1) %>%
  unnest_tokens(word, V1, token = "regex", pattern = "[\\s@,!.?]") ->stop_stem
stop_stem %>% filter(word !="u") -> stop_stem

stop_stem %>% 
  VectorSource() %>%
  VCorpus()%>% 
  DocumentTermMatrix() ->dtm

lda=LDA(dtm, k = 3)
lda.raw=terms(lda,10)
pander(lda.raw, split.table = 180, style = 'rmarkdown', caption = "\\label{tab:LDAraw} Output LDA with the raw data")
```

```{r LDAraw_graph, eval=T, echo=F}
ap_topics <- tidy(lda, matrix = "beta")

ap_top_terms <- ap_topics %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  ungroup() %>%
  arrange(topic, desc(beta)) #%>%View()

split(ap_top_terms, as.factor(ap_top_terms$topic)) %>%
  do.call("cbind",.) %>%
  select(everything(), -contains("topic")) %>%
  pander(split.table = 180, style = 'rmarkdown', caption = "\\label{tab:LDArawp} Word prob. given topic")

plot_ea_topic=function(topic_number){
ap_top_terms %>%
  filter(topic==topic_number) %>%
  mutate(term = reorder(term, beta)) %>%
  ggplot(aes(term, beta))+
  geom_bar(stat = "identity") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
}
lapply(1:3, function(x) plot_ea_topic(x))-> plot_raw
grid.arrange(plot_raw[[1]], plot_raw[[2]], plot_raw[[3]],layout_matrix=cbind(1,2,3))
```



## LDA without Unicode
In most text mining examples, LDA is performed after removing the unicode information. For the first case, therefore, unicode characters were removed from the raw text data set. Then, the standard procedure of stemming and stop word deletion was performed to enhance the accuracy of LDA. `tm` package was used to conduct the above procedure. 

```{r cleaning_function, echo=F, eval=T}
cleaning=function(string){
  string=gsub("U\\+[[:alnum:]]+", "", string)
  string=gsub("u\\+[[:alnum:]]+", "", string)
  string=gsub("&amp;", "", string)
  string=gsub(" {2,}", " ", string)
  string=gsub("#", "", string)
  string=gsub("&lt;", "<", string)   # <3
  return(string)
}
```

```{r LDA_no_uni, echo=F, eval=T}
dat2=character()
dat2$V1<-dat$V1
dat2$V1=cleaning(dat2$V1)

stop_stem=lapply(dat2$V1, StopNStem) %>% 
  do.call("rbind",.) %>% 
  rbind.data.frame(stringsAsFactors=F)

stop_stem %>%
  select(V1) %>%
  unnest_tokens(word, V1, token = "regex", pattern = "[\\s@,!.?]") ->stop_stem

stop_stem %>% 
  VectorSource() %>%
  VCorpus()%>% 
  DocumentTermMatrix() ->dtm

lda=LDA(dtm,control = list(alpha = 0.1), k = 3)
lda.no_uni=terms(lda,5)
pander(lda.no_uni, split.table = 180, style = 'rmarkdown', caption = "\\label{tab:LDA_Uni} Output of LDA with the raw data without the Unicode")
```

```{r LDA_no_uni_graph, eval=T, echo=F}
ap_topics <- tidy(lda, matrix = "beta")

ap_top_terms <- ap_topics %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  ungroup() %>%
  arrange(topic, desc(beta)) #%>% View()

split(ap_top_terms, as.factor(ap_top_terms$topic)) %>%
  do.call("cbind", .) %>%
  select(everything(), -contains("topic")) %>%
  pander(split.table = 180, style = 'rmarkdown', caption = "\\label{tab:LDA_no_unip} Word prob. given topic")

lapply(1:3, function(x) plot_ea_topic(x))-> plot_no_uni
grid.arrange(plot_no_uni[[1]], plot_no_uni[[2]], plot_no_uni[[3]],layout_matrix=cbind(1,2,3))
```



## LDA with name translated
The last case was to perform LDA after traslating the unicode Emoji characters in english. `unicode` package was used to match the unicode to its name. Then the standard process of stemming and deletion of stop words where performed.


```{r LDA_T, echo=F, eval=T}
emoji_simple=read.csv("../data/all_emoji.csv", stringsAsFactors = F)

U_to_N=function(word){
  emoji_simple %>%
    filter(unicode==word)->see_trans
  if(length(see_trans$trans)==0){
    return(word)
  }else{
    return(see_trans$trans)
  }
}
```

```{r aaa, echo=F, eval=T}
check7=NULL
for(j in 1:length(dat$V1)){
  string=dat$V1[j]
  string=gsub(" {2}", " ", string)
  words=strsplit(string, " ")
  words=words[[1]]
  translated=""
  for(i in 1:length(words)){
    translated=paste(translated, U_to_N(words[i]), collapse = " ")
  }
  check7=rbind(check7, tolower(translated))
  # print(j)
}

cbind.data.frame(dat, check7, stringsAsFactors=F)->check8
names(check8)<-c("raw_string", "emoji_present", "string_translated")
# saveRDS(check8, "../Data/twitter_all.Rds")

stop_stem=lapply(check7, StopNStem) %>% 
  do.call("rbind",.) %>% 
  rbind.data.frame(stringsAsFactors=F)

stop_stem %>%
  select(V1) %>%
  unnest_tokens(word, V1, token = "regex", pattern = "[\\s@,!.?]") ->stop_stem

stop_stem %>% 
  VectorSource() %>%
  VCorpus()%>% 
  DocumentTermMatrix() ->dtm

lda=LDA(dtm,control = list(alpha = 0.1), k = 3)
lda.t=terms(lda,5)
pander(lda.t, split.table = 180, style = 'rmarkdown', caption = "\\label{tab:LDA_t} Output of LDA with translated Unicode")
```

```{r LDA_T_graph, eval=T, echo=F}
ap_topics <- tidy(lda, matrix = "beta")

ap_top_terms <- ap_topics %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  ungroup() %>%
  arrange(topic, desc(beta)) # %>% View()

split(ap_top_terms, as.factor(ap_top_terms$topic)) %>%
  do.call("cbind", .) %>%
  select(everything(), -contains("topic")) %>%
  pander(split.table = 180, style = 'rmarkdown', caption = "\\label{tab:LDA_Tp} Word prob. given topic")

lapply(1:3, function(x) plot_ea_topic(x))-> plot_t
grid.arrange(plot_t[[1]], plot_t[[2]], plot_t[[3]],layout_matrix=cbind(1,2,3))
```

# Conclusion
As the result of the exploratory analysis indicates, user-generated-contents may contain Unicode Emoji characters. These Emoji characters sometimes carry mixture of condensed information that is difficult to express in words. The result of the output from the LDA indicates that words such as "heart" that would have been neglected using the traditional method may be saved when the Unicode characters are translated into meanings.