lda.no_uni
stop_stem %>%
gsub("[[:punct:]]", "", .) %>%
gsub("\"", "", .) %>%
gsub("\\s{2,}", " ",.) %>%
trimws() %>%
VectorSource() %>%
VCorpus()%>%
DocumentTermMatrix() ->dtm
lda=LDA(dtm,k = 3)
lda.no_uni=terms(lda,5)
lda.no_uni
ap_topics <- tidy(lda, matrix = "beta")
names(ap_topics)[3]<-"phi"
ap_top_terms <- ap_topics %>%
group_by(topic) %>%
top_n(10, phi) %>%
ungroup() %>%
arrange(topic, desc(phi)) #%>%View()
ap_topics <- tidy(lda, matrix = "beta")
names(ap_topics)[3]<-"phi"
ap_top_terms <- ap_topics %>%
group_by(topic) %>%
top_n(10, phi) %>%
ungroup() %>%
arrange(topic, desc(phi)) #%>% View()
lapply(1:3, function(x) plot_ea_topic(x))-> plot_no_uni
grid.arrange(plot_no_uni[[1]], plot_no_uni[[2]], plot_no_uni[[3]],layout_matrix=cbind(1,2,3))
dat=rbind.data.frame(
readRDS("../Data/twitter_inlove.Rds") %>%
mutate(tag = "inlove"),
readRDS("../Data/twitter_hateher.Rds")%>%
mutate(tag = "hateher"),
readRDS("../data/twitter_marchscience.Rds") %>%
mutate(tag = "marchscience"), stringsAsFactors = F
)
dat$V1 %>%
tolower() %>%
cleaning0() %>%
gsub("[[:punct:]]", "", .) %>%
gsub("\"", "", .)-> dat$V1
stop_stem=lapply(dat$V1, StopNStem) %>%
do.call("rbind",.) %>%
rbind.data.frame(stringsAsFactors=F)
stop_stem=lapply(dat$V1, StopNStem) %>%
do.call("rbind",.) %>%
rbind.data.frame(stringsAsFactors=F)
stop_stem$V1 %>%
gsub("[[:punct:]]", "", .) %>%
gsub("\"", "", .) %>%
gsub("\\s{2,}", " ",.) %>%
trimws() %>%
Vector
dat2=character()
dat2<-dat$V1
dat2=cleaning(dat2)
stop_stem=lapply(dat2, StopNStem) %>%
do.call("rbind",.) %>%
rbind.data.frame(stringsAsFactors=F)
stop_stem %>%
gsub("\\s{2,}", " ",.) %>%
trimws() %>%
VectorSource() %>%
VCorpus()%>%
DocumentTermMatrix() ->dtm
lda=LDA(dtm,k = 3)
lda.no_uni=terms(lda,5)
lda.no_uni
dat$V1
dat=rbind.data.frame(
readRDS("../Data/twitter_inlove.Rds") %>%
mutate(tag = "inlove"),
readRDS("../Data/twitter_hateher.Rds")%>%
mutate(tag = "hateher"),
readRDS("../data/twitter_marchscience.Rds") %>%
mutate(tag = "marchscience"), stringsAsFactors = F
)
dat$V1 %>%
tolower() %>%
cleaning0() %>%
gsub("[[:punct:]]", "", .) %>%
gsub("\"", "", .) %>%
gsub("\\s{2,}", "\\s", .) %>%
trimws-> dat$V1
dat$V1
stop_stem=lapply(dat$V1, StopNStem) %>%
do.call("rbind",.) %>%
rbind.data.frame(stringsAsFactors=F)
stop_stem$V1 %>%
VectorSource() %>%
VCorpus()%>%
DocumentTermMatrix() ->dtm
lda=LDA(dtm, k = 3)
lda=LDA(dtm, k = 3)
lda.raw=terms(lda,10)
lda.raw
dat2=character()
dat2<-dat$V1
dat2=cleaning(dat2)
stop_stem=lapply(dat2, StopNStem) %>%
do.call("rbind",.) %>%
rbind.data.frame(stringsAsFactors=F)
stop_stem %>%
gsub("\\s{2,}", " ",.) %>%
trimws() %>%
VectorSource() %>%
VCorpus()%>%
DocumentTermMatrix() ->dtm
lda=LDA(dtm,k = 3)
lda.no_uni=terms(lda,5)
lda.no_uni
dat2=character()
dat2<-dat$V1
dat2=cleaning(dat2)
stop_stem=lapply(dat2, StopNStem) %>%
do.call("rbind",.) %>%
rbind.data.frame(stringsAsFactors=F)
stop_stem %>%
VectorSource() %>%
VCorpus()%>%
DocumentTermMatrix() ->dtm
lda=LDA(dtm,k = 3)
lda.no_uni=terms(lda,5)
lda.no_uni
cleaning=function(string){
string=gsub("U[[:alnum:]]+", "", string)
string=gsub("u[[:alnum:]]+", "", string)
string=gsub("&amp;", "", string)
string=gsub(" {2,}", " ", string)
string=gsub("#", "", string)
string=gsub("&lt;", "<", string)   # <3
return(string)
}
dat2=character()
dat2<-dat$V1
dat2=cleaning(dat2)
stop_stem=lapply(dat2, StopNStem) %>%
do.call("rbind",.) %>%
rbind.data.frame(stringsAsFactors=F)
stop_stem %>%
VectorSource() %>%
VCorpus()%>%
DocumentTermMatrix() ->dtm
lda=LDA(dtm,k = 3)
terms(lda,5)
ap_topics <- tidy(lda, matrix = "beta")
names(ap_topics)[3]<-"phi"
ap_top_terms <- ap_topics %>%
group_by(topic) %>%
top_n(10, phi) %>%
ungroup() %>%
arrange(topic, desc(phi)) #%>% View()
lapply(1:3, function(x) plot_ea_topic(x))-> plot_no_uni
grid.arrange(plot_no_uni[[1]], plot_no_uni[[2]], plot_no_uni[[3]],layout_matrix=cbind(1,2,3))
emoji_simple=read.csv("../data/all_emoji.csv", stringsAsFactors = F)
U_to_N=function(word){
emoji_simple %>%
filter(unicode==word)->see_trans
if(length(see_trans$trans)==0){
return(word)
}else{
return(see_trans$trans)
}
}
check7=NULL
for(j in 1:length(dat$V1)){
string=dat$V1[j]
words=strsplit(string, " ")
words=words[[1]]
translated=""
for(i in 1:length(words)){
translated=paste(translated, U_to_N(words[i]), collapse = " ")
}
check7=rbind(check7, tolower(translated))
# print(j)
}
emoji_simple
emoji_simple$unicode<-gsub("[[:punct:]]", "", emoji_simple$unicode)
emoji_simple
emoji_simple=read.csv("../data/all_emoji.csv", stringsAsFactors = F)
emoji_simple$unicode<-gsub("[[:punct:]]", "", emoji_simple$unicode)
U_to_N=function(word){
emoji_simple %>%
filter(unicode==word)->see_trans
if(length(see_trans$trans)==0){
return(word)
}else{
return(see_trans$trans)
}
}
check7=NULL
for(j in 1:length(dat$V1)){
string=dat$V1[j]
words=strsplit(string, " ")
words=words[[1]]
translated=""
for(i in 1:length(words)){
translated=paste(translated, U_to_N(words[i]), collapse = " ")
}
check7=rbind(check7, tolower(translated))
print(j)
}
cbind.data.frame(dat, check7, stringsAsFactors=F)->check8
names(check8)<-c("raw_string", "emoji_present", "string_translated")
stop_stem=lapply(check7, StopNStem) %>%
do.call("rbind",.) %>%
rbind.data.frame(stringsAsFactors=F)
stop_stem=lapply(check7, StopNStem) %>%
do.call("rbind",.) %>%
rbind.data.frame(stringsAsFactors=F)
stop_stem %>%
VectorSource() %>%
VCorpus()%>%
DocumentTermMatrix() ->dtm
lda=LDA(dtm,control
lda=LDA(dtm,control
lda=LDA(dtm, k = 3)
lda.t=terms(lda,5)
lda.t
ap_topics <- tidy(lda, matrix = "beta")
naems(ap_topics)[3]<-"phi"
names(ap_topics)[3]<-"phi"
ap_top_terms <- ap_topics %>%
group_by(topic) %>%
top_n(10, phi) %>%
ungroup() %>%
arrange(topic, desc(phi)) # %>% View()
lapply(1:3, function(x) plot_ea_topic(x))-> plot_t
grid.arrange(plot_t[[1]], plot_t[[2]], plot_t[[3]],layout_matrix=cbind(1,2,3))
stop_stem$V1 %>%
VectorSource() %>%
VCorpus()%>%
DocumentTermMatrix() ->check
str(check)
lda@beta
str(lda)
str(check)
lda=LDA(check, k = 3)
str(lda)
as.matrix(lda)
as.matrix(lda@wordassignments)
as.matrix(lda@wordassignments) %>% View()
str(lda)
lda@control@estimate.beta
lda@wordassignments %>% unique() %>% length()
lda@wordassignments
lda@wordassignments$j %>% unique() %>% length()
lda@gamma
str(lda)
dir(-10.97)
ap_topics <- tidy(lda, matrix = "beta")
ap_topics
ap_topics
ddirichlet(x=c("A", "B", "C"), alpha=(0.5, 0.3, 0.2))
ddirichlet(x=c("A", "B", "C"), alpha=c(0.5, 0.3, 0.2))
lda@beta[1,1]
lda@beta
str(lda@beta)
lda@beta[,1]
sum(lda@beta[,1])-lda@beta[1,1]
beta(lda@beta[1,1], sum(lda@beta[,1])-lda@beta[1,1])
exp^lda@beta[1,1]
lda@beta[1,1]
exp(lda@beta[1,1])
exp(sum(lda@beta[,1])-lda@beta[1,1])
beta(exp(lda@beta[1,1]), exp(sum(lda@beta[,1])-lda@beta[1,1]))
log(625039920)
beta(lda@beta[1,1], sum(lda@beta[,1])-lda@beta[1,1])
(sum(lda@beta[,1])-lda@beta[1,1])
lda@beta[1,1]
beta(-1,-1)
beta(exp(lda@beta[1,1]), exp(sum(lda@beta[,1])-lda@beta[1,1]))
beta(exp(lda@beta[1,1]), exp(sum(lda@beta[,1]))-exp(lda@beta[1,1]))
beta(exp(lda@beta[1,1]), (exp(sum(lda@beta[,1]))-exp(lda@beta[1,1])))
(exp(sum(lda@beta[,1]))-exp(lda@beta[1,1]))
beta(exp(lda@beta[1,1]), (exp(sum(lda@beta[,1])-lda@beta[1,1])))
beta(exp(lda@beta[1,1]), (exp(sum(lda@beta[,1]))-exp(lda@beta[1,1])))
beta(exp(lda@beta[1,1]), (exp(sum(lda@beta[,1])-lda@beta[1,1])))
lda@beta[,1]
sum(exp(lda@beta[,1]))
(sum(exp(lda@beta[,1]))-exp(lda@beta[1,1]))
beta(exp(lda@beta[1,1]), (sum(exp(lda@beta[,1]))-exp(lda@beta[1,1])))
exp(lda@beta[1,1])
beta(exp(lda@beta[2,1]), (sum(exp(lda@beta[,1]))-exp(lda@beta[2,1])))
beta(exp(lda@beta[3,1]), (sum(exp(lda@beta[,1]))-exp(lda@beta[3,1])))
beta(exp(lda@beta[4,1]), (sum(exp(lda@beta[,1]))-exp(lda@beta[4,1])))
beta(exp(lda@beta[3,1]), (sum(exp(lda@beta[,1]))-exp(lda@beta[3,1])))
beta(exp(lda@beta[3,1]), (sum(exp(lda@beta[,1]))-exp(lda@beta[3,1]))) %>% log()
str(lda)
lda@gamma
str(lda)
ap_topics
?tidy
lda@beta
lda@beta %>% View()
ap_topics %>% View()
lda@beta %>% View()
lda@beta[,1]
ap_topics %>% View()
log(-10.968041)
exp(-10.968041)
exp(-9.909966)
lda@beta %>% View()
beta(exp(lda@beta[1,1]), (sum(exp(lda@beta[1,]))-exp(lda@beta[1,1])))
exp(lda@beta[1,1])
sum(exp(lda@beta[1,]))
exp(lda@beta[1,1])
(sum(exp(lda@beta[1,]))-exp(lda@beta[1,1])
)
beta(exp(lda@beta[1,1]), (sum(exp(lda@beta[1,]))-exp(lda@beta[1,1])))
?beta
ap_topics %>% View()
sum(exp(lda@beta[2,]))
sum(exp(lda@beta[3,]))
str(lda)
lda@beta %>% View()
rmultinom(10, size = 12, prob = c(0.1,0.2,0.8))
rmultinom(1, size = 12, prob = c(0.1,0.2,0.8))
rmultinom(1, size = 12, prob = c(0.1,0.2,0.8))
rmultinom(1, size = 12, prob = c(0.1,0.2,0.8))
ap_lda <- LDA(AssociatedPress, k = 2, control = list(seed = 1234))
data("AssociatedPress")
ap_lda <- LDA(AssociatedPress, k = 2, control = list(seed = 1234))
str(ap_lda)
ap_lda@beta
ap_lda@beta %>% View()
exp(-27.10812)
exp(-10.15299)
pkg=c("tidytext","tidyverse", "pander","topicmodels", "tm", "gridExtra", "pipeR")
sapply(pkg, require, character=T)
sapply(pkg, require, character=T)
'%!in%' <- function(x,y)!('%in%'(x,y))
twitter_example_inlove=readRDS("../data/twitter_inlove.Rds")
twitter_example_hateher=readRDS("../data/twitter_hateher.Rds")
twitter_example_marchscience=readRDS("../data/twitter_marchscience.Rds")
filter(twitter_example_inlove, grepl("U\\+",V1)) %>%
unnest_tokens(word, V1, token = "regex", pattern = "[\\s]") %>%
select(word) %>%
filter(grepl("u\\+",word)) %>%
count(word, sort = TRUE) %>%
head(5)->inlove_uni_count
filter(twitter_example_inlove, grepl("U\\+",V1)) %>%
unnest_tokens(word, V1, token = "regex", pattern = "[\\s]") %>%
select(word) %>%
filter(grepl("u\\+",word)) %>%
count(word, sort = TRUE) %>%
head(5)->inlove_uni_count
colnames(inlove_uni_count)<-c("#inlove","Count")
filter(twitter_example_hateher, grepl("U\\+",V1)) %>%
unnest_tokens(word, V1, token = "regex", pattern = "[\\s]") %>%
select(word) %>%
filter(grepl("u\\+",word)) %>%
count(word, sort = TRUE) %>%
head(5)->hateher_uni_count
colnames(hateher_uni_count)<-c("#hateher","Count")
filter(twitter_example_marchscience, grepl("U\\+",V1)) %>%
unnest_tokens(word, V1, token = "regex", pattern = "[\\s]") %>%
select(word) %>%
filter(grepl("u\\+",word)) %>%
count(word, sort = TRUE) %>%
head(5)->marchscience_uni_count
colnames(marchscience_uni_count)<-c("#marchscience","Count")
emoji_freq=cbind.data.frame(inlove_uni_count, hateher_uni_count, marchscience_uni_count)
pander(emoji_freq, split.table = 180, style = 'rmarkdown', caption = "\\label{tab:EPopular} Five most popular emoji for each hastags")
StopNStem=function(text){
vdc=VectorSource(text)
my.corpus = Corpus(vdc)
my.corpus_copy = my.corpus
my.corpus = tm_map(my.corpus, removeWords, c("the", stopwords("english")))
my.corpus = tm_map(my.corpus, stemDocument, language="english")
return(paste(strwrap(my.corpus[[1]]), sep="", collapse=""))
}
dat=rbind.data.frame(
readRDS("../Data/twitter_inlove.Rds") %>%
mutate(tag = "inlove"),
readRDS("../Data/twitter_hateher.Rds")%>%
mutate(tag = "hateher"),
readRDS("../data/twitter_marchscience.Rds") %>%
mutate(tag = "marchscience"), stringsAsFactors = F
)
dat$V1 %>%
tolower() %>%
cleaning0() %>%
gsub("[[:punct:]]", "", .) %>%
gsub("\"", "", .) %>%
gsub("\\s{2,}", "\\s", .) %>%
trimws()-> dat$V1
stop_stem=lapply(dat$V1, StopNStem) %>%
do.call("rbind",.) %>%
rbind.data.frame(stringsAsFactors=F)
library(MCMCpack)
stop_stem=lapply(dat$V1, StopNStem) %>%
do.call("rbind",.) %>%
rbind.data.frame(stringsAsFactors=F)
stop_stem$V1 %>%
VectorSource() %>%
VCorpus()%>%
DocumentTermMatrix() ->dtm
stop_stem$V1 %>%
VectorSource() %>%
VCorpus()%>%
DocumentTermMatrix() ->check
str(check)
as.matrix(check)
lda=LDA(check, k = 3)
str(lda)
str(lda@beta)
lda@beta %>% View()
lda@gamma %>% View()
lda@gamma[1,
]
exp(digamma(lda@gamma[1,])
exp(digamma(lda@gamma[1,])-digamma(sum(lda@gamma[1,])))
rdirichlet(16)
rdirichlet(1, 16)
rdirichlet(15, 16)
rdirichlet(15, 30)
rdirichlet(15, 0.1)
lda@gamma[1,]
?LDA alpha
?alpha
??alpha
lda@alpha
log(34)
?beta
?lda
?topicmodels::LDA
dat=rbind.data.frame(
readRDS("../Data/twitter_inlove.Rds") %>%
mutate(tag = "inlove"),
readRDS("../Data/twitter_hateher.Rds")%>%
mutate(tag = "hateher"),
readRDS("../data/twitter_marchscience.Rds") %>%
mutate(tag = "marchscience"), stringsAsFactors = F
)
return(paste(strwrap(my.corpus[[1]]), sep="", collapse=""))
cleaning0=function(string){
string=gsub("&amp;", "", string)
string=gsub("#", "", string)
string=gsub("&lt;", "<", string)   # <3
string=gsub(" [:./[:alnum:]]+.com[/[:alnum:]]*", " ", string)
string=gsub(" http[:./[:alnum:]]+", " ", string)
string=gsub(" {2,}", " ", string)
return(string)
}
StopNStem=function(text){
vdc=VectorSource(text)
my.corpus = Corpus(vdc)
my.corpus_copy = my.corpus
my.corpus = tm_map(my.corpus, removeWords, c("the", stopwords("english")))
return(paste(strwrap(my.corpus[[1]]), sep="", collapse=""))
}
dat$V1
StopNStem(dat$V1[1])
dat$V1[1]
lapply(dat$V1[1:5], StopNStem) %>%
do.call("rbind",.) %>%
rbind.data.frame(Original=dat$V1[1:5], stopword_r=., stringsAsFactors=F)
require(xtable)
xtable(lapply(dat$V1[1:5], StopNStem) %>%
do.call("rbind",.) %>%
rbind.data.frame(Original=dat$V1[1:5], stopword_r=., stringsAsFactors=F), caption = "Before and after removing stop words", label = "tab:stopword")
lapply(dat$V1[1:5], StopNStem) %>%
do.call("rbind",.) %>%
rbind.data.frame(Original=dat$V1[1:5], stopword_r=., stringsAsFactors=F)
lapply(dat$V1[1:5], StopNStem) %>%
do.call("rbind",.) %>%
rbind.data.frame(Original=dat$V1[1:5], stopword_r=., stringsAsFactors=F)-see
lapply(dat$V1[1:5], StopNStem) %>%
do.call("rbind",.) %>%
rbind.data.frame(Original=dat$V1[1:5], stopword_r=., stringsAsFactors=F)->see
View(see)
rbind.data.frame(Original=dat$V1[1:5], stopword_r=lapply(dat$V1[1:5], StopNStem) %>%
do.call("rbind",.), stringsAsFactors=F)->see
View(see)
data.frame(Original=dat$V1[1:5], stopword_r=lapply(dat$V1[1:5], StopNStem) %>%
do.call("rbind",.), stringsAsFactors=F)->see
View(see)
xtable(see, caption = "Before and after removing stop words", label = "tab:stopword")
View(see)
StopNStem=function(text){
vdc=VectorSource(text)
my.corpus = Corpus(vdc)
my.corpus_copy = my.corpus
my.corpus = tm_map(my.corpus, removeWords, c("the", stopwords("english")))
my.corpus = tm_map(my.corpus, stemDocument, language="english")
return(paste(strwrap(my.corpus[[1]]), sep="", collapse=""))
}
data.frame(Original=dat$V1[1:5], stopword_r=lapply(dat$V1[1:5], StopNStem) %>%
do.call("rbind",.), stringsAsFactors=F)->see
xtable(see, caption = "Before and after removing stop words", label = "tab:stopword")
StopNStem=function(text){
vdc=VectorSource(text)
my.corpus = Corpus(vdc)
my.corpus_copy = my.corpus
# my.corpus = tm_map(my.corpus, removeWords, c("the", stopwords("english")))
my.corpus = tm_map(my.corpus, stemDocument, language="english")
return(paste(strwrap(my.corpus[[1]]), sep="", collapse=""))
}
data.frame(Original=dat$V1[1:5], stopword_r=lapply(dat$V1[1:5], StopNStem) %>%
do.call("rbind",.), stringsAsFactors=F)->see
StopNStem=function(text){
vdc=VectorSource(text)
my.corpus = Corpus(vdc)
my.corpus_copy = my.corpus
# my.corpus = tm_map(my.corpus, removeWords, c("the", stopwords("english")))
my.corpus = tm_map(my.corpus, stemDocument, language="english")
return(paste(strwrap(my.corpus[[1]]), sep="", collapse=""))
}
data.frame(Original=dat$V1[1:5], stopword_r=lapply(dat$V1[1:5], StopNStem) %>%
do.call("rbind",.), stringsAsFactors=F)->see
xtable(see, caption = "Before and after Stemming", label = "tab:stemmed")
