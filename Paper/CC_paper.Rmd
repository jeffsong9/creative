---
title: "Latent Dirichlet Allocation Models Considering Emojis"
author: "Taikgun Song"
header-includes:
   - \usepackage{amsmath}
   - \usepackage{graphicx}
   - \usepackage{bbm}
   - \usepackage{subcaption}
   - \usepackage[export]{adjustbox}
   - \usepackage{wrapfig}
   - \usepackage{color}
   - \usepackage{enumerate}
output:
  pdf_document:
    fig_caption: yes
    number_sections: yes
geometry: margin=0.5in
---
```{r hidecode, echo=FALSE}
library(knitr)
opts_chunk$set(echo=F, warning=FALSE, message = FALSE, cache=TRUE, tidy.opts=list(width.cutoff=40))
```

```{r library and source, include=F}
# pkg=c("tidytext","tidyverse", "gsubfn", "Unicode","pander","topicmodels", "tm", "RTextTools", "gridExtra")
pkg=c("tidytext","tidyverse", "pander","topicmodels", "tm", "gridExtra")
sapply(pkg, require, character=T)
'%!in%' <- function(x,y)!('%in%'(x,y))
```

### abstract

XXX write later XXX

\tableofcontents

# Introduction

<!--intelligence, customer care service, social media analysis, contextual advertising, and more.-->
Latent Dirichlet Allocation(LDA) is a popular hierarchical Bayesian model that is widely used as a topic modeling method. LDA exploit statistical inference to discover latent topic of text data, however, the method depends on the number of observations - words. This dependency on observed words lead LDA to its systematic limit is when there exists data sparcity with short text data. New communication medium such as Social Network Service(SNS) and User Generated Content(UGC) platform increased the amount of text data usage, however, the size of the document is limited to a couple hundred words. Hence, LDA model is known for its low performance on these short online text due to the data sparcity.

xxx Bridge xxx

The use of Emoji character - a pictographic information that carries class of feelings -  with other text data is a unique characteristic of online messages. Conventionally, Emoji characters have been considered as a noise and were deleted prior to applying LDA technique. In contrast with the previous procedure, this paper propose the idea of incorporating Emoji characters to enhance the performance of the LDA method on short online texts.

The use of Emoji characters have three main benefits. First, it may reduce the systematic problem of LDA with data sparcity. All Emoji characters have name and keywords associated with the contextual meaning that it conveys. By translating Emoji characters into its English name or related keywords will increase the observation, and thus lead to better LDA results. Second, each Emoji character has a couple of pre-determined topic dimension set by the official organization. This information could be used as an auxiliary information during the topic matching process. Lastly, Emoji character itself is an abstract of emotion and symbolic representation. Thus, it is natural to take the output of LDA containing Emoji translation to sentiment analysis. 

<!--The development of Social Network Service(SNS) and User Generated Content(UGC) platforms have been providing a new channels for its users to communicate and share opinions with the fellow community members. Hundreds and thousands of new messages are shared every day on Twitter and Facebook; product reviews are posted on Amazon and Trip Advisor. The text contents of SNS and UGC is a raw information of Individual's perception and emotion. As one would expect, this distinct characteristic of SNS and UGC text data could be a valuable information in different academic disciplines and industries. As a result, different machine learning and natural language processing techniques were developed to evaluate text data.
Most natural language processing(NLP) techniques require preparation procedures to improve its performance. Deleting stop words(determiners, conjunctions, prepositions, pronouns, auxiliary verbs, modals, and quantifiers) is an example of this preceding step. The result of this step increases the accuracy of the NLP by reducing the noise generated from words that does not convey any contextual meaning. In this context, Emoji(a pictographic information that carries class of feelings) in the text data has been considered as a noise and was deleted prior to applying NLP techniques.
Although deleting Emoji Unicode before NLP is a standard operation, unlike deleting stop words, this does take away information that might have modest contribution to the context. Emoji, originally driven from Japanese word e(picture) + moji(character), is a pictograph that has become widely used on internet web pages and on SNS platforms. Emoji, much like its close relative emoticon, could provide visual representation of not only solid objects, but also emotions through facial expressions and symbols related to feelings and moods. Communication via traditional text characters, such as words in alphabets, may not be as effective and efficient as Emojis when conveying emotions. For this reason, Emoji gained popularity after 1990 especially after cell phones and internet came to wide use. Therefore, simply deleting Emoji would reduce the information contained in the original text data.
In contrast to filtering Emoji characters out, reflecting Emoji information during the evaluation have benefits. First, each Emoji has pre-determined topic dimension set by the official organization. Therefore, crude topic matching may be accomplished using the Emoji information. Second, Emoji may be helpful for sentiment analysis. Being closely related to emotions and mood, sentiment analysis on Emoji will provide auxiliary information. -->

XXX Should the packages used to run example be introduced here with brief steps? XXX
The `tm`, `topicmodels`, `emoji`, `tidytext`, and `tidyverse` package in `R` was written to help the above analysis.



#LDA

LDA is a popular method to infer semantics to model a document as a mixture of latent topics.

LDA is a topic modeling method that allows words observed in documents to be explained by unobserved topics and that each word's creation is attributable to one of the document's topics.

LDA is a topic modeling method that allows words observed in documents to be explained by unobserved topics and that each word's creation is attributable to one of the document's topics.

LDA is based on the two following principles:

\begin{enumerate}
\item Every document is a mixture of topics
\item Every topic is a mixture of words
\end {enumerate}

To illustrate, a news paper document may contain several topics such as "politics", "economy", "spots", "entertainment", and etc. For a given topic "politics", common words may be "government", "trump", "president", "congress", and etc. 

LDA assumes that the probability of documents are random mixture over unseen topics, and document $i$ having topic $k$ follows a Dirichlet distribution with some parameter $\alpha$. That is, if the probability of document $i$ having topic $k$ is denoted as $\theta_{i,k}$, then $\theta_i \sim Dir(\alpha)$. The second assumption says each topic is a mixture of words, and that the distribution of $n^{th}$ word will follow a multinomial distribution conditioned on the topic z. The probability of word given a topic is denoted as $\beta$.  Then $\beta$ has a Dirichlet distribution with parameter $\eta$.

\begin{enumerate}
\item $\theta_i \sim Dir(\alpha), i=1, \dots, M$
\item $\theta_{i,k}$ is the probability that document $i \in \big\{1, \dots, M \big\}$ has topic $k \in \big\{1, \dots, K \big\}$.
\item $z$ is word's topic drawn from a Multinomial distribution with parameter \textbf{$\theta$}, i.e. $z \sim Multi(\theta)$
\item $\beta_k \sim Dir(\eta), k=1, \dots, K$
\item $\beta_{k,v}$ is the probability of word $v \in \big\{1, \dots, V \big\}$ in topic $k \in \big\{1, \dots, K \big\}$
\item $w$ is a word drawn from a Multinomial distribution with parameter Z and $\beta$, i.e., $w \sim Multi(z, \beta)$.
\end{enumerate}

The marginal distribution of word w given hyper parameter $\alpha$ and $\beta$ is obtained by the following equation:

$$p(w|\alpha, \beta)= \int p(\theta|\alpha)(\prod_{v=1}^{V} \sum_{z_{v}} p(z_v|\theta)p(w_v|z_v, \beta))d\theta$$
where 

Graphical display of LDA is given in \autoref{fig:LDA}.

\begin{figure}[htbp]
\centering
\includegraphics[width=100mm]{"LDA_model".png}
\caption{Graphical Model representation of LDA \label{fig:LDA}}
\end{figure}

##LDA Equation goes here



As indicated in the above section, LDA assumes that documents are represented as random mixtures over latent topics and each topic is characterized by a distribution over words. Therefore, the frequency of each word influence the outcome of the LDA.

## Removing Stop Words

A natural language can be categorized as two distinctive set of words: content/lexical words and function/structure words. Content/lexical words are words with substantive meanings. Function/structure words on the other hand have little lexical meaning, but establish grammatical structure between other words within a sentence. 

LDA models a document as a mixture of topics, and then each word is drawn from one of its topic. Therefore, the method depends on the frequency of observed words in a given text data set. This makes LDA method vulnerable when meaningless words such as function/structural words are present in the data set with high freqeuncy. Thus, any group of non-informative words including the function/structural words should be filtered out before processing LDA method, and this group of words are called the `stop words`. For example, prepositions(of, at, in, without, between), determiners(the, a, that, my), conjunctions(and, that, when), pronouns(he, they, anybody, it) are common examples of the `stop words`. For the work done in the paper, the `tm` package in `R` was used to delete stop words.


<!--Stop words are a group of natural language words that are essential in construction of grammatical structure of English. For example, words such as "a", "an", "the", "that", "these", "my", "his", "most", "with", and \textit{et cetera} is an example of stop words. Although these words are crucial elements of English structure, however, these stop words are less important when it comes to the purpose of delivering the meanings. Also, high frequency and abundance nature of these stop words makes little value in helping text mining techniques, and thus be filtered out.-->




## Stemming

Due to structural and grammatical reasons of English, a family of words that are driven from a single root word is used in different forms. For example, words such as "stems", "stemmer", "stemming", and "stemmed" are all based on a root word "stem". <!--These different form of words with the same root word may cause difficulties applying some text mining techniques.--> Words with same meaning but different in forms contribute to data sparcity, reducing the performance of the LDA method. The `stemming` procedure cuts inflectional forms of a word to its root form eventually increasing the frequency of word observations.

The stemming process has two disadvantages. First, there are possibility of over stemming. For example, three different words "universal", "university", and "universe" have the same stemmed word "univers". The accuracy of the LDA method may decrease by putting words with different meanings into a single topic. Moreover, when the LDA output is given as a stemmed word, it is difficult to trace the stemmed word to its original form.

XXX Explain why we cannot trace back to the original form XXX

The `tm` package is again used for the stemming process and its code is given as the following.




# Application

## Data Set and exploratory data analysis

Two samples of twitter messages with the following hash-tag #inlove and #hateher were scraped. The data set contains 944 #inlove messages, 1145 #hateher messages, and 1195 #marchscience messages. The proportion of Twitter messages containing Emoji characters per hashtag is illustraited in \autoref{tab:EProp}. 52.7% of the #inlove message strings, 29.3% of the #hateher message strings, and 7.8% of #marchscience message strings have one or more emoji information.

```{r load_twitter_data, eval=T, echo=F}
twitter_example_inlove=readRDS("../data/twitter_inlove.Rds")
twitter_example_hateher=readRDS("../data/twitter_hateher.Rds")
twitter_example_marchscience=readRDS("../data/twitter_marchscience.Rds")
```

```{r, echo=F}
prop_emoji=cbind.data.frame(498/944, 335/1145, 93/1195)
colnames(prop_emoji)<-c("#inlove", "#hateher", "#marchscience")
rownames(prop_emoji)<-"Proportion"

pander(prop_emoji, split.table = 180, style = 'rmarkdown', caption = "\\label{tab:EProp} Proportion of Twitter messages with Emoji")
```

```{r, eval=F, echo=F}
filter(twitter_example_inlove, grepl("U\\+",V1)) %>%
  unnest_tokens(word, V1, token = "regex", pattern = "[\\s]") %>%
  select(word) %>%
  filter(grepl("u\\+",word)) %>%
  count(word, sort = TRUE) %>%
  head(5)->inlove_uni_count
colnames(inlove_uni_count)<-c("#inlove","Count")

filter(twitter_example_hateher, grepl("U\\+",V1)) %>%
  unnest_tokens(word, V1, token = "regex", pattern = "[\\s]") %>%
  select(word) %>%
  filter(grepl("u\\+",word)) %>%
  count(word, sort = TRUE) %>%
  head(5)->hateher_uni_count
colnames(hateher_uni_count)<-c("#hateher","Count")

filter(twitter_example_marchscience, grepl("U\\+",V1)) %>%
  unnest_tokens(word, V1, token = "regex", pattern = "[\\s]") %>%
  select(word) %>%
  filter(grepl("u\\+",word)) %>%
  count(word, sort = TRUE) %>%
  head(5)->marchscience_uni_count
colnames(marchscience_uni_count)<-c("#marchscience","Count")

emoji_freq=cbind.data.frame(inlove_uni_count, hateher_uni_count, marchscience_uni_count)

pander(emoji_freq, split.table = 180, style = 'rmarkdown', caption = "\\label{tab:EPopular} Five most popular Emoji for each hastags")

# xdate_number=xtable(emoji_freq, caption = "Five most popular Emoji for each hastags", label = "tab:EPopular")
# print(xdate_number)
```

For hashtag #inlove, total number of 1188 Emojis were used from 182 unique emojis. 
For hashtag #hateher, 695 Emojis from 112 unique Emojis were used.
For hashtag #sciencemarch, 202 Emojis from 102 unique Emojis were used (Note that there may be multiple Emojis per Twitter message).
Top 5 frequently used Emojis per hashtag is given in \autoref{tab:EPopular}.

\begin{table}[htbp]
\centering
\begin{tabular}{ccccccccc}
  \hline
  \#inlove & Emoji & Count & \#hateher & Emoji & Count & \#marchscience & Emoji & Count \\ 
  \hline
U+1F60D & \includegraphics[width=0.03\textwidth, height=60mm]{images/U+1F60D.png} & 297 & U+1F602 & \includegraphics[width=0.03\textwidth, height=60mm]{images/U+1F602.png} & 154 & U+1F52C &  \includegraphics[width=0.03\textwidth, height=60mm]{images/U+1F52C.png} & 13 \\ 
U+2764 & \includegraphics[width=0.03\textwidth, height=60mm]{images/U+2764.png} & 164 & U+1F644 &  \includegraphics[width=0.03\textwidth, height=60mm]{images/U+1F644.png} & 88 & U+1F30E &  \includegraphics[width=0.03\textwidth, height=60mm]{images/U+1F30E.png} & 11 \\ 
U+1F495 & \includegraphics[width=0.03\textwidth, height=60mm]{images/U+1F495.png} & 47 & U+1F621 &  \includegraphics[width=0.03\textwidth, height=60mm]{images/U+1F621.png} & 40 & U+1F44D &   \includegraphics[width=0.03\textwidth, height=60mm]{images/U+1F44D.png} & 9 \\ 
U+1F618 & \includegraphics[width=0.03\textwidth, height=60mm]{images/U+1F618.png} & 40 & U+1F612 &  \includegraphics[width=0.03\textwidth, height=60mm]{images/U+1F612.png} & 38 & U+1F680 &   \includegraphics[width=0.03\textwidth, height=60mm]{images/U+1F680.png} & 8 \\ 
U+2728 & \includegraphics[width=0.03\textwidth, height=60mm]{images/U+2728.png} & 26 & U+1F62D &  \includegraphics[width=0.03\textwidth, height=60mm]{images/U+1F62D.png} & 36 & U+1F30D &   \includegraphics[width=0.03\textwidth, height=60mm]{images/U+1F30D.png} & 7 \\ 
   \hline
\end{tabular}
\caption{Five most popular Emoji for each hastags} 
\label{tab:EPopular}
\end{table}

<!--
\begin{figure}[htbp]
\centering
\includegraphics[width=150mm]{"pop_emoji".png}
\caption{Five most popular Emoji for each hastags \label{fig:EPopular}}
\end{figure}
-->

It is interesting to see "Face with tear of joy" as the most popular Emoji for hashtag #hateher. Although the name itself contains the word "joy", some users of this Emoji adopted this pictogram to express their mixed feeling of love and hate at the same time.



##Application

LDA was performed on the following three difference cases:

\begin{enumerate}
\item LDA on a raw data set
\item LDA on a data set with Unicode removed
\item LDA on a data set with Emoji translated to text
\end{enumerate}


```{r application setup, echo=F, eval=T}
StopNStem=function(text){
  vdc=VectorSource(text)
  my.corpus = Corpus(vdc)
  my.corpus_copy = my.corpus
  my.corpus = tm_map(my.corpus, removeWords, c("the", stopwords("english"))) 
  my.corpus = tm_map(my.corpus, stemDocument, language="english")
  return(paste(strwrap(my.corpus[[1]]), sep="", collapse=""))
}

cleaning0=function(string){
  string=gsub("&amp;", "", string)
  string=gsub("#", "", string)
  string=gsub("&lt;", "<", string)   # <3
  string=gsub(" [:./[:alnum:]]+.com[/[:alnum:]]*", " ", string)
  string=gsub(" http[:./[:alnum:]]+", " ", string)
  string=gsub(" {2,}", " ", string)
  return(string)
}

dat=rbind.data.frame(
  readRDS("../Data/twitter_inlove.Rds") %>% 
    mutate(tag = "inlove"),
  readRDS("../Data/twitter_hateher.Rds")%>% 
    mutate(tag = "hateher"),
  readRDS("../data/twitter_marchscience.Rds") %>%
    mutate(tag = "marchscience"), stringsAsFactors = F
  )

dat$V1 %>% tolower() %>%  cleaning0()-> dat$V1
```

## LDA on a raw data set
The second case was to run LDA on a raw data set. Stemming and stop word deletion were performed. Different number of topic dimensions were tested and the result of 4 topic dimension with 10 terms are provided in \autoref{tab:LDAraw}. Describe the output.


```{r LDAraw, echo=F, eval=T}
stop_stem=lapply(dat$V1, StopNStem) %>% 
  do.call("rbind",.) %>% 
  rbind.data.frame(stringsAsFactors=F)

stop_stem %>%
  select(V1) %>%
  unnest_tokens(word, V1, token = "regex", pattern = "[\\s@,!.?]") ->stop_stem
stop_stem %>% filter(word !="u") -> stop_stem

stop_stem %>% 
  VectorSource() %>%
  VCorpus()%>% 
  DocumentTermMatrix() ->dtm

lda=LDA(dtm, k = 3)
lda.raw=terms(lda,10)
pander(lda.raw, split.table = 180, style = 'rmarkdown', caption = "\\label{tab:LDAraw} Output LDA with the raw data")
```

```{r LDAraw_graph, eval=T, echo=F}
ap_topics <- tidy(lda, matrix = "beta")

ap_top_terms <- ap_topics %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  ungroup() %>%
  arrange(topic, desc(beta)) #%>%View()

split(ap_top_terms, as.factor(ap_top_terms$topic)) %>%
  do.call("cbind",.) %>%
  select(everything(), -contains("topic")) %>%
  pander(split.table = 180, style = 'rmarkdown', caption = "\\label{tab:LDArawp} Word prob. given topic")

plot_ea_topic=function(topic_number){
ap_top_terms %>%
  filter(topic==topic_number) %>%
  mutate(term = reorder(term, beta)) %>%
  ggplot(aes(term, beta))+
  geom_bar(stat = "identity") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
}
lapply(1:3, function(x) plot_ea_topic(x))-> plot_raw
grid.arrange(plot_raw[[1]], plot_raw[[2]], plot_raw[[3]],layout_matrix=cbind(1,2,3))
```



## LDA without Unicode
In most text mining examples, LDA is performed after removing the Unicode information. For the first case, therefore, Unicode characters were removed from the raw text data set. Then, the standard procedure of stemming and stop word deletion was performed to enhance the accuracy of LDA. `tm` package was used to conduct the above procedure. 

```{r cleaning_function, echo=F, eval=T}
cleaning=function(string){
  string=gsub("U\\+[[:alnum:]]+", "", string)
  string=gsub("u\\+[[:alnum:]]+", "", string)
  string=gsub("&amp;", "", string)
  string=gsub(" {2,}", " ", string)
  string=gsub("#", "", string)
  string=gsub("&lt;", "<", string)   # <3
  return(string)
}
```

```{r LDA_no_uni, echo=F, eval=T}
dat2=character()
dat2$V1<-dat$V1
dat2$V1=cleaning(dat2$V1)

stop_stem=lapply(dat2$V1, StopNStem) %>% 
  do.call("rbind",.) %>% 
  rbind.data.frame(stringsAsFactors=F)

stop_stem %>%
  select(V1) %>%
  unnest_tokens(word, V1, token = "regex", pattern = "[\\s@,!.?]") ->stop_stem

stop_stem %>% 
  VectorSource() %>%
  VCorpus()%>% 
  DocumentTermMatrix() ->dtm

lda=LDA(dtm,control = list(alpha = 0.1), k = 3)
lda.no_uni=terms(lda,5)
pander(lda.no_uni, split.table = 180, style = 'rmarkdown', caption = "\\label{tab:LDA_Uni} Output of LDA with the raw data without the Unicode")
```

```{r LDA_no_uni_graph, eval=T, echo=F}
ap_topics <- tidy(lda, matrix = "beta")

ap_top_terms <- ap_topics %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  ungroup() %>%
  arrange(topic, desc(beta)) #%>% View()

split(ap_top_terms, as.factor(ap_top_terms$topic)) %>%
  do.call("cbind", .) %>%
  select(everything(), -contains("topic")) %>%
  pander(split.table = 180, style = 'rmarkdown', caption = "\\label{tab:LDA_no_unip} Word prob. given topic")

lapply(1:3, function(x) plot_ea_topic(x))-> plot_no_uni
grid.arrange(plot_no_uni[[1]], plot_no_uni[[2]], plot_no_uni[[3]],layout_matrix=cbind(1,2,3))
```



## LDA with name translated
The last case was to perform LDA after translating the Unicode Emoji characters in English. `unicode` package was used to match the Unicode to its name. Then the standard process of stemming and deletion of stop words where performed.


```{r LDA_T, echo=F, eval=T}
emoji_simple=read.csv("../data/all_emoji.csv", stringsAsFactors = F)

U_to_N=function(word){
  emoji_simple %>%
    filter(unicode==word)->see_trans
  if(length(see_trans$trans)==0){
    return(word)
  }else{
    return(see_trans$trans)
  }
}
```

```{r aaa, echo=F, eval=T}
check7=NULL
for(j in 1:length(dat$V1)){
  string=dat$V1[j]
  string=gsub(" {2}", " ", string)
  words=strsplit(string, " ")
  words=words[[1]]
  translated=""
  for(i in 1:length(words)){
    translated=paste(translated, U_to_N(words[i]), collapse = " ")
  }
  check7=rbind(check7, tolower(translated))
  # print(j)
}

cbind.data.frame(dat, check7, stringsAsFactors=F)->check8
names(check8)<-c("raw_string", "emoji_present", "string_translated")
# saveRDS(check8, "../Data/twitter_all.Rds")

stop_stem=lapply(check7, StopNStem) %>% 
  do.call("rbind",.) %>% 
  rbind.data.frame(stringsAsFactors=F)

stop_stem %>%
  select(V1) %>%
  unnest_tokens(word, V1, token = "regex", pattern = "[\\s@,!.?]") ->stop_stem

stop_stem %>% 
  VectorSource() %>%
  VCorpus()%>% 
  DocumentTermMatrix() ->dtm

lda=LDA(dtm,control = list(alpha = 0.1), k = 3)
lda.t=terms(lda,5)
pander(lda.t, split.table = 180, style = 'rmarkdown', caption = "\\label{tab:LDA_t} Output of LDA with translated Unicode")
```

```{r LDA_T_graph, eval=T, echo=F}
ap_topics <- tidy(lda, matrix = "beta")

ap_top_terms <- ap_topics %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  ungroup() %>%
  arrange(topic, desc(beta)) # %>% View()

split(ap_top_terms, as.factor(ap_top_terms$topic)) %>%
  do.call("cbind", .) %>%
  select(everything(), -contains("topic")) %>%
  pander(split.table = 180, style = 'rmarkdown', caption = "\\label{tab:LDA_Tp} Word prob. given topic")

lapply(1:3, function(x) plot_ea_topic(x))-> plot_t
grid.arrange(plot_t[[1]], plot_t[[2]], plot_t[[3]],layout_matrix=cbind(1,2,3))
```

# Conclusion
As the result of the exploratory analysis indicates, user-generated-contents may contain Unicode Emoji characters. These Emoji characters sometimes carry mixture of condensed information that is difficult to express in words. The result of the output from the LDA indicates that words such as "heart" that would have been neglected using the traditional method may be saved when the Unicode characters are translated into meanings.


# Appendix

# `emoji` package in R
\textbf{Plan to change this part after posting the Emoji package on CRAN}

## Description of the Emoji package

The `Emoji` package contains information of the Emoji v5.0 from its official publisher the \href{http://unicode.org/emoji/charts/emoji-list.html}{Unicode Consortium}. The illustration of the web page is shown in \autoref{fig:uni_web}.

\begin{figure}[htbp]
\centering
\includegraphics[width=100mm]{"unicode_webpage".png}
\caption{Glimpse of the table of Emoji on the Unicode.org website \label{fig:uni_web}}
\end{figure}

The data set `emoji` in the `Emoji` package contains 8 variables:

uni_no: Official number of emojis$\\$
uni_code: Formal Unicode of emojis$\\$
uni_name: Official name of emojis$\\$
cat1: Official category of emojis$\\$
cat2: Official sub-category of emojis from cat1$\\$
cat3: Official sub-category of emojis from cat2$\\$
uni_keyws: Official keyword(s) of emojis$\\$
uni_png: Image of emojis in PNG format represented in a matrix format$\\$

The package has a function `emoji_info_table` that summarizes all Emoji and their information used in a single character string.


## Scoring of Sentiment

The characteristic of Emoji (effectively delivers feelings and moods), naturally leads text mining with Emoji to sentiment analysis. `tidytext` package in R has three general purpose lexicon sets. The `AFINN` score words from -5 to 5 scale, `bing` assigns words in binary category(positive and negative), and `nrc`assigns words with more categories. 



```{r show_pkg_info, echo=F}
library(emoji)
sample_string="Obviously #hateher  U+1f495  U+1f60f  U+1f469  @ Los Angeles, California https://www.instagram.com/p/BCbPSxxGVua/"
emoji_info_table(sample_string) %>%
pander(split.table = 180, style = 'rmarkdown', caption = "\\label{tab:pkg}Example of the Emoji package")
```



```{r show_dataset, echo=F, eval=F}
uni_info=readRDS("../data/uni_info_v8.Rds")
egdat=head(uni_info,5)
edgat=egdat[,-c(6:7,13)]

pander(edgat, split.table = 180, style = 'rmarkdown', caption = "\\label{tab:dataset}Information of 5 Emoji data set")
```


# More work
1. Check Stemming - scienc vs. science
2. Check output again. Also, a check aggregation of short messages to avoid data sparsity.
3. LDA explanation
4. Description of the Emoji package
