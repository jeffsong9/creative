---
title: "Latent Dirichlet Allocation Models Considering Emojis"
author: "Taikgun Song"
data: 'system.time'
header-includes:
- \usepackage{amsmath}
- \usepackage{graphicx}
- \usepackage{bbm}
#- \usepackage{subcaption}
- \usepackage[export]{adjustbox}
- \usepackage{wrapfig}
- \usepackage{color}
- \usepackage{xcolor}
- \usepackage{enumerate}
- \usepackage{subfig}
output:
  pdf_document:
    fig_caption: yes
    number_sections: yes
documentclass: article
classoption: letterpaper
#geometry: margin=1in
---

\newcommand{\hh}[1]{{\color{orange} #1}}
\newcommand{\ts}[1]{\textcolor{blue}{#1}}

```{r hidecode, echo=FALSE}
library(knitr)
opts_chunk$set(echo=F, warning=FALSE, message = FALSE, cache=T, tidy.opts=list(width.cutoff=40))
```

```{r library_and_source, include=F}
source("req.R")
library(ggrepel)
Sys.setlocale('LC_ALL','C')
load("../data/emojidata.rda")
```

\begin{abstract}

Latent Dirichlet Allocation (LDA) is a popular topic modeling technique for natural language processing. Emoji is a pictogram that is commonly used on Social Networking Service (SNS) such as Twitter and Instagram. Conventionally, emoji characters were deleted before applying LDA to text data from SNS. In this research, we compared the performance of the LDA under three different cases: (1) LDA with text data including emojis as unicode characters, (2) LDA with emoji characters excluded from the data, and (3) LDA after trasnalting the emoji characters into English. Using Twitter text messages of three different hastags, our analyses revealed that case (1) LDA with text data including emojis as unicode characters identified the topics the best. Our result highlights that an emoji character aggregates information of n-gram words into a uni-gram word which significantly enhances the performance of LDA by preventing the loss of information during the exclusion or translation process.
\end{abstract}

\tableofcontents

# Introduction


Text data contains valuable insights that is useful for content recommendation, customer care service, social media analysis, and others.  However, the information is usually hidden within the text and has to be extracted using a modeling approach. Topic modeling is a text-mining method that extracts information from a text by identifying latent semantic structures in the text body. One of the most widely used topic modeling methods is the Latent Dirichlet Allocation(LDA).  LDA is a hierarchical Bayesian model which assumes that each of the documents in a collection consists of a mixture of topics, and these topics are responsible for the choice of words in each document. Topics, are the latent part of the document set and one can only observe words collected in the documents. LDA uses statistical inference to discover structure given the words and documents by calculating the relative importance of topics in documents and words in topics.


The rapid growth in internet and telecommunication technology triggered the development of Social Network Services(SNS) platform such as Tweeter, Facebook, and blog posts. The SNS messages often include individual's perceptions, feelings, and opinions. Evaluating this data may be meaningful for policy makers, social science researchers, and business entrepreneurs. This electronic word-of-mouth heavily uses text data as the medium of communication. Thus, topic modeling including LDA may be ideal method for analyzing SNS text data for information retrieval tasks. 

The use of emoji - a pictogram that expresses the author's feeling and emotion - mixed in with other text is a unique characteristic of SNS messages that distinguishes itself from other text data. As shown in \autoref{fig:twe}, many SNS messages can be found with emoji embedded in the content. Conventionally, emoji characters have been considered as a noise and were deleted prior to applying LDA techniques and other topic modeling methods. Nevertheless, one should focus on the richness of information that emoji characters can provide. Especially consider the emotional and symbolic representation of emoji that cannot be better expressed with alphabet characters. Therefore, in contrast to the typical topic modeling procedure, this paper proposes the idea of incorporating emoji characters to enhance the performance of the LDA method on SNS text data.


\begin{figure}[htbp]
\centering
\includegraphics[width=100mm]{"images/t_w_E".png}
\caption{Example of Twitter Messages \label{fig:twe}}
\end{figure}


The use of emoji characters has three main benefits. First, it reduces the systematic problem of LDA with data sparsity. All emoji characters have name and keywords associated with the contextual meaning that it conveys. By translating emoji characters into English text and related keywords increases the amount of the text observed, and thus leads to better LDA results. Second, each emoji character has a set of pre-determined topic dimension assigned to it by the official organization. This information can be used as auxiliary information during the topic matching process. Lastly, the emoji character itself is an abstract of emotion and symbolic representation. Thus, it is natural to take the output of LDA containing emoji translation to sentiment analysis. 

# Latent Dirichlet Allocation (LDA)

At its core Latent Dirichlet Allocation is a generative statistical model, that identifies posterior probabilities of words belonging to previously unidentified topics, and topics belonging to documents. The underlying model is generally not analytically tractable, but uses a Gibbs sampling approach instead. Here, we are deriving the  posterior distributions involved in more detail.
A graphical overview of LDA is given in \autoref{fig:LDA}.
\begin{figure}[htbp]
\centering
\includegraphics[width=100mm]{"images/LDA_model".png}
\caption{Graphical model representation of LDA \label{fig:LDA} in plate notation.}
\end{figure}

Let $M$ be the total number of documents in the data set, and $N_m$ be the number of words in the $m^{th}$ document. Let $K$ be the total number of topics in the data. Define $w_{mn}$ be the $n^{th}$ word in the $m^{th}$ document. LDA assumes the distribution of $w_{mn}$ to follow a Multinomial distribution with parameter $\phi_{z, w}$. $\phi_{z, w}$ is a probability of observing word $w$ in topic $z$. The model assumes that the distribution of words in topic $z$, i.e., $\boldsymbol\phi_{z}$, follows a Dirichlet distribution with prior $\boldsymbol\beta = \big[\beta_1 \cdots \beta_N \big]$. Let $z_mn$ be the topic of assigned to the word $w_{mn}$. Then, the model assumes $\boldsymbol{z_m}$ to follow a Multinomial distribution with parameter $\boldsymbol\theta_m$, where $\boldsymbol\theta_m$ is the distribution of topics in document $m$. The distribution of $\boldsymbol\theta_m$ is assumed to follow a Dirichlet distribution with a prior $\boldsymbol{\alpha} = \big[\alpha_1 \cdots \alpha_K \big]$. 

Let $w_{mn}$ be the $n^{th}$ word in the $m^{th}$ document. We assume that the topic of $w_{mn}$ is $z_{m}$, a topic associated with document $m$.  Assume $z_m \sim Multinomial(\boldsymbol\theta_m)$, where $\boldsymbol\theta_m \sim Dirichlet(\boldsymbol\alpha)$ for all $m=1, \dots M$ and $\alpha>0$. For a given topic $z_m=k$, we assume that $w_{mn} \sim Multinomial(\boldsymbol\phi_k), n=1, \dots, n_m, m=1, \dots, M$, where $\boldsymbol\phi_k \sim Dirichlet(\boldsymbol\beta)$, $k=1, \dots, K$.

The summarization of the assumptions are written below.

\begin{enumerate}
\item $M$: The total number of documents in the data set
\item $N_m$: The number of words in the $m^{th}$ document
\item $K$: The total number of topics in the data set
\item $w_{mn}$: $n^{th}$ word in document $m$, $m \in \big\{1, \dots, M\big\}$ and $n \in \big\{1, \cdots, N_m\big\}$
\item $z_{mn}$: The topic of the $w_{mn}$, $z_{mn} \in \big\{1, \dots, K \big\}$
\item $\boldsymbol\alpha$: A vector of prior weights for each topic in a document\\
\quad $\boldsymbol\alpha$ = \big[$\alpha_1 \cdots \alpha_K$ \big]
\item $\theta_{m, k}$: The probability of observing topic $k$ in document $m$\\
$\boldsymbol\theta_m \sim Dir(\boldsymbol\alpha)$: The distribution of topics in document $m$\\
\quad $\boldsymbol\theta_{M \times K}=
\begin{bmatrix}
\boldsymbol\theta_1= (\theta_{1,1}, \theta_{1,2}, \cdots, \theta_{1,K})\\
\boldsymbol\theta_2= (\theta_{2,1}, \theta_{2,2}, \cdots, \theta_{2,K})\\
\vdots\\
\boldsymbol\theta_M
\end{bmatrix}
$
\item $\boldsymbol\beta$: A vector of prior weights of the word distribution for each topic\\
\quad $\boldsymbol\beta$ = \big[$\beta_1 \cdots \beta_N$ \big]
\item $\phi_{z, w}$: The probability of observing word $w$ in topic $z$\\
$\boldsymbol\phi_z \sim Dir(\boldsymbol\beta)$: The distribution of words in topic $z$\\
\quad $\boldsymbol\phi_{K \times N}=
\begin{bmatrix}
\boldsymbol\phi_1= (\phi_{1,1}, \phi_{1,2}, \cdots, \phi_{1,N})\\
\boldsymbol\phi_2= (\phi_{2,1}, \phi_{2,2}, \cdots, \phi_{2,N})\\
\vdots\\
\boldsymbol\phi_K
\end{bmatrix}
$
\item $z_{mn} \sim Multinomial(\theta_m)$
\item $w_{mn} \sim Multinomial(\phi_{z_{mn}})$
\end{enumerate}


Then, the total probability of the model is given as the product of the conditional probabilities
$$p(W, Z, \theta; \phi, \alpha, \beta)=\prod_{i=1}^{K}P(\phi_i;\beta)\prod_{j=1}^{M}P(\theta_j;\alpha)\prod_{t=1}^{N}P(Z_{j,t}|\theta_j)P(W_{j,t}|\phi z_{j,t})$$

The marginal distribution of word w given hyper parameter $\alpha$ and $\beta$ is then obtained by integrating the below equation:
$$p(w|\alpha, \beta)= \int p(\theta|\alpha) \left(\prod_{v=1}^{V} \sum_{z_{v}} p(z_v|\theta)p(w_v|z_v, \beta) \right)d\theta$$

The posterior distribution is given as the following equation, however, it is intractable for exact inference and Gibbs sampling is used to infer the variables.

$$p(\boldsymbol\theta, \textbf{z} | \textbf{w}, \alpha, \beta)= \frac{p(\theta, z, w | \alpha, \beta)}{p(w|\alpha, \beta)}$$


# Data preparation

## Removing Stop Words

A natural language can be categorized as two distinctive set of words: content/lexical words and function/structure words. Content/lexical words are words with substantive meanings. Function/structure words on the other hand have little lexical meaning, but establish grammatical structure between other words within a sentence. 

LDA models a document as a mixture of topics, and then each word is drawn from one of its topic. Therefore, the method depends on the frequency of observed words in a given text data set. This makes LDA vulnerable to high frequency function/structural words. Thus, any group of non-informative words including the function/structural words should be filtered out before doing an analysis. This group of words is called `stop words`. For example, prepositions(of, at, in, without, between), determiners(the, a, that, my), conjunctions(and, that, when), pronouns(he, they, anybody, it) are common examples of the `stop words`. For the analysis done here, the `tm` package in `R` was used to delete the stop words.


\begin{table}[ht]
\centering
\begin{tabular}{rp{.4\textwidth}p{.4\textwidth}}
  \hline
 & Original Tweet & Tweet with Stopword Removed \\ 
  \hline
1 & loving this misty weather this sweater and my favorite couple & loving misty weather, sweater favorite couple\\ 
  2 & fairytale atmosphere in alberobello Let's go for a walk& fairytale atmosphere alberobello Let's go walk\\ 
  3 & Me when ashleytisdale puts a New music session on YouTube & Me ashleytisdale puts New music session YouTube\\ 
   \hline
\end{tabular}
\caption{Example of removing stop words using the Twitter data}
\label{tab:stopword}
\end{table}




## Stemming

Due to structural and grammatical reasons of English, a family of words that are driven from a single root word is used in different forms. For example, words such as "stems", "stemmer", "stemming", and "stemmed" are all based on the root "stem".  Words with the same meaning but different forms contribute to data sparsity, reducing the performance of the LDA method. `Stemming` cuts inflectional forms of a word to its root form and increases the frequency of observed stems.

Stemming has two disadvantages. First, there is the possibility of over stemming. For example, three different words "universal", "university", and "universe" have the same stemmed word "univers". The accuracy of the LDA method may decrease by putting words with different meanings into a single topic. Moreover, when the LDA output is given as a stemmed word, it is difficult to trace the stemmed word back to its original form. To overcome this problem, this paper matched the stemmed word to the most frequently used original word. Example of stemming  using the `tm` is provided in \autoref{tab:stemmed}.

\begin{table}[ht]
\centering
\begin{tabular}{rp{.4\textwidth}p{.4\textwidth}}
  \hline
 & Original Tweet & Tweet after Stemming \\ 
  \hline
1 & loving this misty weather, this sweater and my favorite couple & love this misti weather, this sweater and my favorit coupl\\ 
  2 & fairytale atmosphere in alberobello  Let's go for a walk & fairytal atmospher in alberobello Let go for a walk\\ 
  3 & Me when ashleytisdale puts a New music session on YouTube & Me when ashleytisdal put a New music session on YouTub\\ 
   \hline
\end{tabular}
\caption{Before and after Stemming} 
\label{tab:stemmed}
\end{table}


## n-gram
n-gram is a neighboring sequence of n items from a collection of text data set. This item could be anything from phonemes or syllables to letters or words based on the application. Applying the concept of n-gram is important in computational linguistics is important especially with LDA, since n-gram is used as part of the prior distribution.

An example of word-level-n-gram with text "he is a nice person" is given in \autoref{tab:ngram}.

\begin{table}[ht]
\centering
\begin{tabular}{ccccc}
  \hline
 1-gram (unigram) & 2-gram & 3-gram & 4-gram & 5-gram\\ 
  \hline
he & he is & he is a & he is a nice & he is a nice person\\ 
is & is a & is a nice & is a nice person &\\ 
a & a nice & a nice person & &\\ 
nice & nice person & & &\\ 
person &  & & &\\ 
   \hline
\end{tabular}
\caption{Example of word-level-n-gram}
\label{tab:ngram}
\end{table}

Moreover, n-gram approach can help identify misspelled words or out-of-vocabulary words that commonly exist on the online platform. For example, the distance of the letter-level n-gram could be used to match strings.


# Application

## Data Set and exploratory data analysis

Three samples of Twitter messages with the following hash-tag #inlove, #hateher, and #marchscience were scraped. The data set contains 944 #inlove messages, 1145 #hateher messages, and 1195 #marchscience messages. The proportion of Twitter messages containing emoji characters per hashtag is illustrated in \autoref{tab:EProp}. 52.7% of the #inlove tweets, 29.3% of the #hateher tweets, and 7.8% of #marchscience tweets make use of one or more emojis.

```{r load_twitter_data, eval=T, echo=F}
twitter_example_inlove=readRDS("../data/twitter_inlove.Rds")
twitter_example_hateher=readRDS("../data/twitter_hateher.Rds")
twitter_example_marchscience=readRDS("../data/twitter_marchscience.Rds")
```

```{r, echo=F}
prop_emoji=cbind.data.frame(498/944, 335/1145, 93/1195)
colnames(prop_emoji)<-c("#inlove", "#hateher", "#marchscience")
rownames(prop_emoji)<-"Proportion"
pander(prop_emoji, split.table = 180, style = 'rmarkdown', caption = "\\label{tab:EProp} Proportion of Twitter messages with emoji")
```

```{r, eval=F, echo=F}
filter(twitter_example_inlove, grepl("U\\+",V1)) %>%
  unnest_tokens(word, V1, token = "regex", pattern = "[\\s]") %>%
  select(word) %>%
  filter(grepl("u\\+",word)) %>%
  count(word, sort = TRUE) %>%
  head(5)->inlove_uni_count
colnames(inlove_uni_count)<-c("#inlove","Count")

filter(twitter_example_hateher, grepl("U\\+",V1)) %>%
  unnest_tokens(word, V1, token = "regex", pattern = "[\\s]") %>%
  select(word) %>%
  filter(grepl("u\\+",word)) %>%
  count(word, sort = TRUE) %>%
  head(5)->hateher_uni_count
colnames(hateher_uni_count)<-c("#hateher","Count")

filter(twitter_example_marchscience, grepl("U\\+",V1)) %>%
  unnest_tokens(word, V1, token = "regex", pattern = "[\\s]") %>%
  select(word) %>%
  filter(grepl("u\\+",word)) %>%
  count(word, sort = TRUE) %>%
  head(5)->marchscience_uni_count
colnames(marchscience_uni_count)<-c("#marchscience","Count")

emoji_freq=cbind.data.frame(inlove_uni_count, hateher_uni_count, marchscience_uni_count)

pander(emoji_freq, split.table = 180, style = 'rmarkdown', caption = "\\label{tab:EPopular} Five most popular emoji for each hastags")
```

For the hashtag #inlove, a total number of 1188 emojis were used, consisting of 182 unique emojis. 
For hashtag #hateher, 695 emojis from 112 unique emojis were used.
For hashtag #sciencemarch, 202 emojis from 102 unique emojis were used<!--(Note that there may be multiple emojis per Twitter message)-->.
Top 5 frequently used emojis per hashtag is given in \autoref{tab:EPopular}. It was interesting to see "Face with tear of joy" as the most popular emoji for hashtag #hateher. Although the name itself contains the word "joy", some users of this emoji adopted this pictogram to express their mixed feeling of love and hate at the same time.

\begin{table}[htbp]
\centering
\begin{tabular}{ccccccccc}
  \hline
  \#inlove & emoji & Count & \#hateher & emoji & Count & \#marchscience & emoji & Count \\ 
  \hline
U+1F60D & \includegraphics[width=0.03\textwidth, height=60mm]{images/U+1F60D.png} & 297 & U+1F602 & \includegraphics[width=0.03\textwidth, height=60mm]{images/U+1F602.png} & 154 & U+1F52C &  \includegraphics[width=0.03\textwidth, height=60mm]{images/U+1F52C.png} & 13 \\ 
U+2764 & \includegraphics[width=0.03\textwidth, height=60mm]{images/U+2764.png} & 164 & U+1F644 &  \includegraphics[width=0.03\textwidth, height=60mm]{images/U+1F644.png} & 88 & U+1F30E &  \includegraphics[width=0.03\textwidth, height=60mm]{images/U+1F30E.png} & 11 \\ 
U+1F495 & \includegraphics[width=0.03\textwidth, height=60mm]{images/U+1F495.png} & 47 & U+1F621 &  \includegraphics[width=0.03\textwidth, height=60mm]{images/U+1F621.png} & 40 & U+1F44D &   \includegraphics[width=0.03\textwidth, height=60mm]{images/U+1F44D.png} & 9 \\ 
U+1F618 & \includegraphics[width=0.03\textwidth, height=60mm]{images/U+1F618.png} & 40 & U+1F612 &  \includegraphics[width=0.03\textwidth, height=60mm]{images/U+1F612.png} & 38 & U+1F680 &   \includegraphics[width=0.03\textwidth, height=60mm]{images/U+1F680.png} & 8 \\ 
U+2728 & \includegraphics[width=0.03\textwidth, height=60mm]{images/U+2728.png} & 26 & U+1F62D &  \includegraphics[width=0.03\textwidth, height=60mm]{images/U+1F62D.png} & 36 & U+1F30D &   \includegraphics[width=0.03\textwidth, height=60mm]{images/U+1F30D.png} & 7 \\ 
   \hline
\end{tabular}
\caption{Five most popular emoji for each hashtag} 
\label{tab:EPopular}
\end{table}

Two hashtag #inlove and #hateher were selected since the two topics should contain the opposite sentiments. The hastag #sciencemarch was used as a control topic, thus making all hashtags distinctive. If the performance of the LDA is effective, then the composition words assgined to each latent topic should have similar characteristics based on the three initial hastags.

\pagebreak

##Results

LDA was performed on the following three difference cases:

\begin{enumerate}
\item LDA on a data set with emoji characters deleted
\item LDA on a data set with emoji characters as unicode
\item LDA on a data set with emoji translated into English
\end{enumerate}

A ternary plot is a triangular graph that displays three variables with respect to its proportion that sum to one. Ternary plots were used to illustrate the LDA ouput of the three different cases listed above. Each corner of the triangle represents the topic determined by the LDA method. The conditional probability of a topic given a word was calculated for all words in the corpus. The calcualted conditional probability was used as the proportion to construct the ternary plot.


```{r application setup, echo=F, eval=T}
dat=rbind.data.frame(
  readRDS("../data/twitter_inlove.Rds") %>% .$V1 %>% paste0(collapse=" "),
    # mutate(tag = "inlove"),
  readRDS("../data/twitter_hateher.Rds")%>% .$V1 %>% paste0(collapse=" "),
    # mutate(tag = "hateher"),
  readRDS("../data/twitter_marchscience.Rds") %>%.$V1 %>% paste0(collapse=" "),
  stringsAsFactors = F
    # mutate(tag = "marchscience"), stringsAsFactors = F
  )
names(dat)<-"V1"

dat$V1 %>>% 
  tolower() %>>%
  clean_abb() %>>%
  cleaning0() %>%  
  gsub("[[:punct:]]", "", .) %>%
  gsub("\"", "", .) %>%
  gsub("\\s{2,}", "\\s", .) %>%
  trimws()-> dat1
```

<!--## LDA on a raw data set MARKER-->

```{r LDAraw, echo=F, eval=T}
stop_stem=lapply(dat1, StopNStem) %>% 
  do.call("rbind",.) %>% 
  as.data.frame(stringsAsFactors=F)

stop_stem$V1 %>%
  VectorSource() %>%
  VCorpus()%>% 
  DocumentTermMatrix() ->dtm

lda=LDA(dtm, k = 3, control = list(alpha=1))
lda.raw=terms(lda,10)
# pander(lda.raw, split.table = 180, style = 'rmarkdown', caption = "\\label{tab:LDAraw} Output LDA with the raw data")->LDAraw
```


```{r LDAraw_graph,  eval=T, echo=F}
ap_topics <- tidy(lda, matrix = "beta")
names(ap_topics)[3]<-"phi"

ap_top_terms <- ap_topics %>%
  group_by(topic) %>%
  top_n(10, phi) %>%
  ungroup() %>%
  arrange(topic, desc(phi)) #%>%View()

# ap_top_terms<-ap_top_terms[-11,] #Two topic 1 with same phi
# 
 # split(ap_top_terms, as.factor(ap_top_terms$topic)) %>%
 #  do.call("cbind",.) %>%
 #  select(everything(), -contains("topic")) %>%
 #  mutate(`1.phi`=gsub("^(.{6}).*","\\1",as.character(`1.phi`)),`2.phi`=gsub("^(.{6}).*","\\1",as.character(`2.phi`)), `3.phi`=gsub("^(.{6}).*","\\1",as.character(`3.phi`))) %>%
 #   xtable()
 #  pander(split.table = 180, style = 'rmarkdown', caption = "\\label{tab:LDArawp} Word prob. given topic") %>%#->LDArawp

# plot_ea_topic=function(topic_number, y_lim){
# ap_top_terms %>%
#   filter(topic==topic_number) %>%
#   mutate(term = reorder(term, phi)) %>%
#   ggplot(aes(term, phi))+
#   geom_bar(stat = "identity") +
#   theme(axis.text.x = element_text(angle = 90, hjust = 1))+
#   ylab(expression(phi))+
#   ylim(0, y_lim)
# }
# lapply(1:3, function(x) plot_ea_topic(x, 0.1))-> plot_raw
# grid.arrange(plot_raw[[1]], plot_raw[[2]], plot_raw[[3]], nrow=1)#,layout_matrix=cbind(1,2,3))
ap_top_terms %>%
  mutate(order = row_number())->ap_top_terms2
ap_top_terms2 %>>%
  ggplot(aes(x = order, y = phi))+ 
  geom_bar(stat='identity', na.rm=T)+ 
  # facet_grid(~topic, scales = "free", space = "free") +
  facet_wrap(~topic, scales = "free") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))+
  ylab(expression(phi))+
  scale_x_continuous(breaks = ap_top_terms2$order, labels = ap_top_terms2$term, expand = c(0,0))->raw_bar
```

```{r LDAraw_ternary, eval=T, echo=F}
ap_topics %>%
  group_by(term) %>%
  mutate(sum_prob=sum(phi)) %>%
  mutate(per_word_per_topic=phi/sum_prob) %>%
  ungroup()->ap_per_word_per_topic

ap_per_word_per_topic %>%
  select(topic, term, per_word_per_topic) %>%
  spread(topic, per_word_per_topic)->plot_this

plot_this %>%
  filter(term %in% {ap_top_terms$term %>% unique()})->use_this
ggtern(data=use_this, aes(x = `1`, y = `2`, z = `3`)) +
  geom_point(color = 'red') +
  # geom_point(aes(fill = term),size = 3,shape = 21)+ 
  # ggtitle(“LDA RAW”) +
  theme_rgbw() +
  theme(legend.position = c(0,1),legend.justification = c(1, 1))+
  geom_text(data=use_this, aes(x = `1`, y = `2`, label=term))->tern_plot_text#, z = `3`, label=term))
ggtern(data=use_this, aes(x = `1`, y = `2`, z = `3`)) +
  geom_point(aes(),size = 3,shape = 21)+ 
  # geom_point(aes(fill = term),size = 3,shape = 21)+ 
  # ggtitle(“LDA RAW”) +
  theme_rgbw() +
  theme(legend.position = c(0,1),legend.justification = c(1, 1))->tern_plot_no_text
crd=coord_tern()
emojis=use_this[grepl("^u[[:alnum:]]{4,5}$", use_this$term),] %>% as.data.frame(stringsAsFactors=F)
names(emojis)<-c("term", "x", "y", "z")
emojis %>%
  select(-term) %>%
  tlr2xy(crd) %>%
  cbind.data.frame(emojis,.) %>%
  .[,c(1,5,6)]->emojis
test<-emojidata$uni_png[[min(which(emojidata$uni_code %in% gsub("U","U+",toupper(emojis$term[2]))))]]
```

```{r ternary_plot_via_ggtern, eval=F}
save_raster_tern<-tern_plot_no_text
for(i in 1:nrow(emojis)){
  save_raster_tern+
  emojidata$uni_png[[min(which(emojidata$uni_code %in% gsub("U","U+",toupper(emojis$term[i]))))]] %>%
    annotation_raster_tern(
      xmin =floor(emojis$x[i]*10)/10, 
      xmax =ceiling(emojis$x[i]*10)/10, 
      ymin =floor(emojis$y[i]*10)/10, 
      ymax = ceiling(emojis$y[i]*10)/10,
      interpolate = FALSE)->save_raster_tern
}
save_raster_tern
# tern_plot_no_text+
#   annotation_raster_tern(test, xmin = 0.3, xmax = .4, ymin = 0.7, ymax =0.8,
# interpolate = FALSE)
```


```{r ternary_plot_via_ggplot2, fig.pos='h', fig.cap="\\label{fig:raw_tern}Ternary plot of the LDA output including the raw unicode characters"}
use_this2<-as.data.frame(use_this)
names(use_this2)<-c("term", "x", "y", "z")
# g<-rasterGrob(test, interpolate = T)
# use_this2%>%
#   select(-term) %>%
#   tlr2xy(crd)%>%
#   cbind(use_this) %>%
#   ggplot(aes(x=x, y=y))+
#   geom_point()+
#   annotation_custom(g, xmin = 0.4, xmax = 0.5, ymin = 0.7, ymax = 0.8)+
#   theme(aspect.ratio = 0.9)
save_annotation_custom=use_this2%>%
  select(-term) %>%
  tlr2xy(crd)%>%
  cbind(use_this) %>%
  ggplot(aes(x=x, y=y))+
  geom_point(aes(x=x, y=y), color = 'red') +
  ggrepel::geom_text_repel(aes(x=x, y=y, label = term))+
  theme_bw()+
  theme(aspect.ratio = 0.9,
        axis.title.x=element_blank(),
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank(),
        axis.title.y=element_blank(),
        axis.text.y=element_blank(),
        axis.ticks.y=element_blank(),
        panel.border=element_blank(),
        panel.grid.major=element_blank(),
        panel.grid.minor=element_blank())
for(i in 1:nrow(emojis)){
  save_annotation_custom+
  emojidata$uni_png[[min(which(emojidata$uni_code %in% gsub("U","U+",toupper(emojis$term[i]))))]] %>%
    grid::rasterGrob(interpolate = T) %>%
    annotation_custom(
      xmin =floor(emojis$x[i]*10)/10, 
      xmax =ceiling(emojis$x[i]*10)/10, 
      ymin =floor(emojis$y[i]*10)/10, 
      ymax = ceiling(emojis$y[i]*10)/10)->save_annotation_custom
}
tri=data.frame(x=c(0, 0.5, 1, 0), y=c(0, 1, 0, 0))
save_annotation_custom+
  geom_path(data=tri, aes(x, y))->fig_raw_tern
```


<!--## LDA without Unicode MARKER-->

```{r LDA_no_uni, echo=F, eval=T}
dat$V1 %>>% 
  tolower() %>>%
  cleaning_u_plus() %>>%
  clean_abb() %>>%
  cleaning0() %>%  
  gsub("[[:punct:]]", "", .) %>%
  gsub("\"", "", .) %>%
  gsub("\\s{2,}", "\\s", .) %>%
  trimws()->dat2

stop_stem=lapply(dat2, StopNStem) %>% 
  do.call("rbind",.) %>% 
  as.data.frame(stringsAsFactors=F)

stop_stem %>%
  VectorSource() %>%
  VCorpus()%>% 
  DocumentTermMatrix() ->dtm

lda=LDA(dtm,k = 3, control = list(alpha=1))
lda.no_uni=terms(lda,5)
# pander(lda.no_uni, split.table = 180, style = 'rmarkdown', caption = "\\label{tab:LDA_no_uni} Output of LDA with the raw data without the Unicode")->LDA_no_uni
```

```{r LDA_no_uni_graph, eval=T, echo=F}
ap_topics <- tidy(lda, matrix = "beta")
names(ap_topics)[3]<-"phi"

ap_top_terms <- ap_topics %>%
  group_by(topic) %>%
  top_n(10, phi) %>%
  ungroup() %>%
  arrange(topic, desc(phi)) #%>% View()

split(ap_top_terms, as.factor(ap_top_terms$topic)) %>%
  do.call("cbind", .) %>%
  select(everything(), -contains("topic")) %>%
    mutate(`1.phi`=gsub("^(.{6}).*","\\1",as.character(`1.phi`)),`2.phi`=gsub("^(.{6}).*","\\1",as.character(`2.phi`)), `3.phi`=gsub("^(.{6}).*","\\1",as.character(`3.phi`))) ->LDA_no_unip#%>%
  # pander(split.table = 180, style = 'rmarkdown', caption = "\\label{tab:LDA_no_unip} Word prob. given topic")->LDA_no_unip

# lapply(1:3, function(x) plot_ea_topic(x, 0.06))-> plot_no_uni
# grid.arrange(plot_no_uni[[1]], plot_no_uni[[2]], plot_no_uni[[3]], nrow=1)#,layout_matrix=cbind(1,2,3))
ap_top_terms %>%
  mutate(order = row_number())->ap_top_terms2
ap_top_terms2 %>>%
  ggplot(aes(x = order, y = phi))+ 
  geom_bar(stat='identity', na.rm=T)+ 
  # facet_grid(~topic, scales = "free", space = "free") +
  facet_wrap(~topic, scales = "free") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))+
  ylab(expression(phi))+
  scale_x_continuous(breaks = ap_top_terms2$order, labels = ap_top_terms2$term, expand = c(0,0))->no_bar
```


```{r LDA_no_uni_ternary, eval=T, echo=F}
ap_topics %>%
  group_by(term) %>%
  mutate(sum_prob=sum(phi)) %>%
  mutate(per_word_per_topic=phi/sum_prob) %>%
  ungroup()->ap_per_word_per_topic

ap_per_word_per_topic %>%
  select(topic, term, per_word_per_topic) %>%
  spread(topic, per_word_per_topic)->plot_this

plot_this %>%
  filter(term %in% {ap_top_terms$term %>% unique()})->use_this
# ggtern(data=use_this, aes(x = `1`, y = `2`, z = `3`)) +
#   # geom_point(aes(fill = term),size = 3,shape = 21)+ 
#   # ggtitle(“LDA RAW”) +
#   theme_rgbw() +
#   theme(legend.position = c(0,1),legend.justification = c(1, 1))+
#   geom_text(data=use_this, aes(x = `1`, y = `2`, label=term))
```

```{r no_ternary_plot_via_ggplot2, echo=F, fig.pos='h', fig.cap="\\label{fig:no_tern}"}
use_this2<-as.data.frame(use_this)
names(use_this2)<-c("term", "x", "y", "z")
save_annotation_custom=use_this2%>%
  select(-term) %>%
  tlr2xy(crd)%>%
  cbind(use_this) %>%
  ggplot(aes(x=x, y=y))+
  geom_point(aes(x=x, y=y), color = 'red') +
  ggrepel::geom_text_repel(aes(x=x, y=y, label = term))+
  theme_bw()+
  theme(aspect.ratio = 0.9,
        axis.title.x=element_blank(),
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank(),
        axis.title.y=element_blank(),
        axis.text.y=element_blank(),
        axis.ticks.y=element_blank(),
        panel.border=element_blank(),
        panel.grid.major=element_blank(),
        panel.grid.minor=element_blank())
tri=data.frame(x=c(0, 0.5, 1, 0), y=c(0, 1, 0, 0))
save_annotation_custom+
  geom_path(data=tri, aes(x, y))->fig_no_tern
```

<!--## LDA with name translated MARKER-->

```{r LDA_T, echo=F, eval=T}
emoji_simple=read.csv("../data/all_emoji.csv", stringsAsFactors = F)
# emoji_simple$unicode<-gsub("[[:punct:]]", "", emoji_simple$unicode)

U_to_N=function(word){
  emoji_simple %>%
    filter(unicode==word)->see_trans
  if(length(see_trans$trans)==0){
    return(word)
  }else{
    return(see_trans$trans)
  }
}
```

```{r aaa, echo=F, eval=T}
check7=NULL
for(j in 1:length(dat$V1)){
  string=dat$V1[j] %>% tolower()
  words=strsplit(string, " ")
  words=words[[1]]
  translated=""
  for(i in 1:length(words)){
    translated=paste(translated, U_to_N(words[i]), collapse = " ")
    # print(paste0(j,"-", i))
  }
  check7=rbind(check7, translated)
  # print(j)
}

check7 %>%
  clean_abb() %>>%
  cleaning0() %>%  
  gsub("[[:punct:]]", "", .) %>%
  gsub("\"", "", .) %>%
  gsub("\\s{2,}", "\\s", .) %>%
  trimws()-> check7

# cbind.data.frame(dat, check7, stringsAsFactors=F)->check8
# names(check8)<-c("raw_string", "emoji_present", "string_translated")
# saveRDS(check8, "../Data/twitter_all.Rds")

stop_stem=lapply(check7, StopNStem) %>% 
  do.call("rbind",.) %>% 
  rbind.data.frame(stringsAsFactors=F)

stop_stem %>%
  VectorSource() %>%
  VCorpus()%>% 
  DocumentTermMatrix() ->dtm

lda=LDA(dtm, k = 3, control = list(alpha=1))
lda.t=terms(lda,5)
# pander(lda.t, split.table = 180, style = 'rmarkdown', caption = "\\label{tab:LDA_t} Output of LDA with translated Unicode")->LDA_t
```

```{r LDA_T_graph, eval=T, echo=F}
ap_topics <- tidy(lda, matrix = "beta")
names(ap_topics)[3]<-"phi"

ap_top_terms <- ap_topics %>%
  group_by(topic) %>%
  top_n(10, phi) %>%
  ungroup() %>%
  arrange(topic, desc(phi)) # %>% View()

split(ap_top_terms, as.factor(ap_top_terms$topic)) %>%
  do.call("cbind", .) %>%
  select(everything(), -contains("topic")) %>%
    mutate(`1.phi`=gsub("^(.{6}).*","\\1",as.character(`1.phi`)),`2.phi`=gsub("^(.{6}).*","\\1",as.character(`2.phi`)), `3.phi`=gsub("^(.{6}).*","\\1",as.character(`3.phi`)))->LDA_Tp #%>%
  # pander(split.table = 180, style = 'rmarkdown', caption = "\\label{tab:LDA_Tp} Word prob. given topic")->LDA_Tp

# lapply(1:3, function(x) plot_ea_topic(x, 0.06))-> plot_t
# grid.arrange(plot_t[[1]], plot_t[[2]], plot_t[[3]], nrow=1)#,layout_matrix=cbind(1,2,3))



ap_top_terms %>%
  mutate(order = row_number())->ap_top_terms2
ap_top_terms2 %>>%
  ggplot(aes(x = order, y = phi))+ 
  geom_bar(stat='identity', na.rm=T)+ 
  # facet_grid(~topic, scales = "free", space = "free") +
  facet_wrap(~topic, scales = "free") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))+
  ylab(expression(phi))+
  scale_x_continuous(breaks = ap_top_terms2$order, labels = ap_top_terms2$term, expand = c(0,0))->trans_bar
```

```{r LDA_T_ternary, eval=T, echo=F}
ap_topics %>%
  group_by(term) %>%
  mutate(sum_prob=sum(phi)) %>%
  mutate(per_word_per_topic=phi/sum_prob) %>%
  ungroup()->ap_per_word_per_topic
# saveRDS(ap_per_word_per_topic, "ap_per_word_per_topic.rds")
# readRDS("ap_per_word_per_topic.rds")->ap_per_word_per_topic

ap_per_word_per_topic %>%
  select(topic, term, per_word_per_topic) %>%
  spread(topic, per_word_per_topic)->plot_this
# plot_this[complete.cases(plot_this), ]->plot_this
# plot_this$`1`<-as.numeric(plot_this$`1`)
# plot_this$`2`<-as.numeric(plot_this$`2`)
# plot_this$`3`<-as.numeric(plot_this$`3`)

plot_this %>%
  filter(term %in% {ap_top_terms$term %>% unique()})->use_this
 # ggtern(data=use_this, aes(x = `1`, y = `2`, z = `3`)) +
 # geom_point(aes(fill = term),size = 3,shape = 21)+
 # ggtitle(“LDA RAW”) +
 # theme_rgbw() +
 # theme(legend.position = c(0,1),legend.justification = c(1, 1))+
 # geom_text(data=use_this, aes(x = `1`, y = `2`, label=term))
```

```{r trans_ternary_plot_via_ggplot2, echo=F, fig.pos='h', fig.cap="\\label{fig:trans_tern}"}
use_this2<-as.data.frame(use_this)
names(use_this2)<-c("term", "x", "y", "z")
save_annotation_custom=use_this2%>%
  select(-term) %>%
  tlr2xy(crd)%>%
  cbind(use_this) %>%
  ggplot(aes(x=x, y=y))+
  geom_point(aes(x=x, y=y), color = 'red') +
  ggrepel::geom_text_repel(aes(x=x, y=y, label = term))+
  theme_bw()+
  theme(aspect.ratio = 0.9,
        axis.title.x=element_blank(),
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank(),
        axis.title.y=element_blank(),
        axis.text.y=element_blank(),
        axis.ticks.y=element_blank(),
        panel.border=element_blank(),
        panel.grid.major=element_blank(),
        panel.grid.minor=element_blank())
tri=data.frame(x=c(0, 0.5, 1, 0), y=c(0, 1, 0, 0))
save_annotation_custom+
  geom_path(data=tri, aes(x, y))->fig_trans_tern
```

<!--## MARKER START-->

```{r test1, out.width='.33\\linewidth', fig.width=3.5, fig.height=3.5, fig.cap='\\label{fig:TPlot}LDA Output displayed as a ternary plot for each methods', eval=T, fig.subcap=c('\\label{fig:trans_tern} Emoji translated in English', '\\label{fig:no_tern} All emoji characters excluded', '\\label{fig:raw_tern} All emoji characters included'),  fig.pos='h', fig.scap=NA}
fig_trans_tern
fig_no_tern
fig_raw_tern
```

The first Figure\autoref{fig:trans_tern} have words clustered at the center of the ternary plot. The second Figure \autoref{fig:no_tern} have words spreaded our more than Figure\autoref{fig:trans_tern}, but not as much as the Figure \autoref{fig:raw_tern}. The last Figure\autoref{fig:raw_tern} have four distinctive clusters. Three at each end point of the triangle, and a small cluster at the center. The ternary plot in \ref{fig:TPlot} indicate that the performance of the LDA on the last \ref{fig:raw_tern} was the best since it successfully assigned words with similar characteristics to each laten topics, and a group of general words at the center of the ternary plot.


\pagebreak

### LDA with emoji characters deleted
The current standard practice in natural language processing removes emoji characters from the text data. For the first case, therefore, all emoji characters provided as unicode were removed from the data set before performing LDA. The result of the LDA with three topic dimensions is provided in \autoref{tab:LDA_no_unip}. The bar-chart in \autoref{fig:no_bar} demonstrated that the LDA method did not successfully distinguish the three topics well. The words in Topic 1 turned out to be a mixture of 'Hate Her' and 'Science March', Topic 2 had all three topics, and Topic 3 was associated with 'Hate Her' and 'In Love'. Ternary plot constructed in \autoref{fig:no_tern} showed that the output from the LDA was located at the center of the ternary plot. This indicates that words are weakly connected to all topic dimensions supporting that LDA was not effective.

```{r, echo=F}
# pander(lda.no_uni, split.table = 180, style = 'rmarkdown', caption = "\\label{tab:LDA_no_uni} Output of LDA with the raw data without the Unicode")
LDA_no_unip %>% head(5) %>% pander(split.table = 180, style = 'rmarkdown', caption = "\\label{tab:LDA_no_unip} LDA output of conditional probability of word given topic when emoji characters are deleted")
```

```{r, fig.cap="\\label{fig:no_bar} Graphical display of Table 6", fig.pos='h', fig.height=4, echo=F}
no_bar
```

\pagebreak

### LDA with emoji translated in English
Each emoji characters can be translated into a multi-gram English word. Thus, LDA was practiced on the data with English translated version of the emoji characters. The output of LDA with translation is provided in \autoref{tab:LDA_Tp}. The bar-chart in \autoref{fig:trans_bar} revealed that the performance of the LDA method was ineffective.  Words from 'Hate Her' and 'Science March' were assigned to Topic 1, top words in Topic 2 were from 'In Love', 'Hate Her', and 'Science March', and top words in Topic 3 were composed with words from 'Hate Her' and 'In Love'. The ternary plot in \autoref{fig:trans_tern} also showed that there were no strong relationship between the words and a specific topic. 

```{r, echo=F}
# pander(lda.t, split.table = 180, style = 'rmarkdown', caption = "\\label{tab:LDA_t} Output of LDA with translated Unicode")
LDA_Tp %>%  head(5) %>% pander(split.table = 180, style = 'rmarkdown', caption = "\\label{tab:LDA_Tp} Conditional probability of word given topic when emoji characters are translated into English")
```

```{r, fig.cap="\\label{fig:trans_bar} Graphical display of Table 7", fig.pos='h', fig.height=4,echo=F}
trans_bar
```

\pagebreak

### LDA with emoji characters as unicode
For the last case, LDA was performed with the data set including emoji characters as unicode. The \autoref{tab:LDAraw} and \autoref{fig:raw_bar} showed that LDA successfully distinguished the corpus into three different topics. The bar chart illustrated that Topic 1 is related to 'Science March', Topic 2 is related to 'Hate her', and Topic 3 is related to 'In Love'. In order to visualize the probability of a word given topic, a ternary plot was constructed in \autoref{fig:raw_tern}.  The ternary plot showed that four different groups of words: three groups of words were strongly affiliated with one of the topics. These groups of words were clustered at the end of each vertices. The other group of words was located at the center of the triangle which represented words that are not associated with a specific topic. According to this ternary plot, emoji characters had strong connection with a particular topic.

```{r echo=F}
# pander(lda.raw, split.table = 180, style = 'rmarkdown', caption = "\\label{tab:LDAraw} Output LDA with the raw data")
```

\begin{table}[ht]
\centering
\begin{tabular}{rllllll}
  \hline
 & 1.term & 1.phi & 2.term & 2.phi & 3.term & 3.phi \\ 
  \hline
1 & inlov & 0.0910 & sciencemarch & 0.0881 & hateh & 0.1003 \\ 
  2 & \includegraphics[width=0.02\textwidth, height=35mm]{images/U+1F60D.png} & 0.0302 & scienc & 0.0322 & \includegraphics[width=0.02\textwidth, height=60mm]{images/U+1F602.png} & 0.0139 \\ 
  3 & love & 0.0241 & march & 0.0182 & just & 0.0108 \\ 
  4 & \includegraphics[width=0.02\textwidth, height=35mm]{images/U+2764.png} & 0.0163 & will & 0.0092 & \includegraphics[width=0.02\textwidth, height=35mm]{images/U+1F644.png} & 0.0085 \\ 
  5 & beauti & 0.0073 & join & 0.0087 & get & 0.0061 \\ 
  6 & just & 0.0062 & marchforsci & 0.0082 & like & 0.0061 \\ 
  7 & new & 0.0060 & saturday & 0.0081 & bitch & 0.0055 \\ 
  8 & \includegraphics[width=0.02\textwidth, height=35mm]{images/U+1F495.png} & 0.0049 & sign & 0.0063 & cbb & 0.0042 \\ 
  9 & happi & 0.0045 & april & 0.0058 & one & 0.0040 \\ 
  10 & inlove… & 0.0043 & scientist & 0.0056 & want & 0.0039 \\ 
   \hline
\end{tabular}
\caption{LDA output including emoji characters as unicode} 
\label{tab:LDAraw}
\end{table}

```{r, fig.cap="\\label{fig:raw_bar} Graphical display of Table 8", fig.pos='h', fig.height=4, echo=F}
raw_bar
```

\pagebreak

# Conclusion and Discussion 
Using Twitter text messages of three different hastags, we examined the performance of LDA considering three different cases with emoji characters. Overall, our results from \ref{fig:TPlot} suggest that LDA with emoji characters embedded in the text data as unicode performed the best. \ref{fig:raw_tern} was not only able to assign similar words to each latent topic but also determined terms that were used among all topics such as "just", "one", and "like" and displayed at the center of the plot. <!--While the LDA when all emojis were deleted and the LDA method when all emojis were translated genertated ternary plots \autoref{fig:trans_tern} and  \autoref{fig:no_tern} where words were clusterd at the center of the graphs. This indicate that the LDA method was not able to assign words into three unknown topics.  On the other hand, \autoref{fig:raw_tern} showed that three clusters of words were plotted at each end point of the triangle for method when raw unicode characters were used for the LDA method. These plots indicate that LDA performed the best when the raw unicode characters were used for the analysis.-->

One explanation of the outcome is that translating emoji characters into English affects the performance of LDA by introducing new words with sparsity issue or words that could be assigned to multiple different topics. Emoji characters aggregate -- otherwise separated -- n-gram words into a single character. For example, commonly used Unicode characters in the data set such as 'U+1F602', 'U+1F644', 'U+1F60D', 'U+1F618', 'U+2764', and 'U+1F495' can be translated to English as 'face with tears of joy', 'face with rolling eyes', 'smiling face with heart eyes', 'face blowing a kiss', 'red heart', and 'two hearts' respectively. 

The translation approach introduces new words, and even after removing stopwords and stemming, the distribution of words in the data set changes. This approach may increase the number of words with insignificant meanings or it may introduce data sparsity for words that are important for topic modeling. As alluded to previously, the multi-gram translations share words such as 'face' for multiple emojis that may not have the same characteristic. Since LDA methods are affected by the frequency of words in the data set, the effect of the translation method as aforementioned above may affect the performance of the LDA as shown in case three. Since only uni-gram-level LDA was examined in this paper, a multi-gram-level LDA with English translated emoji maybe considered as a future research topic.

Moreover, deleting emoji characters will lead to information loss in the text data. Emoji are rich in contextual information, thus deleting the entire emoji characters lead to a greater information loss and consequently affected the output of the LDA. The comparison of the \ref{fig:no_tern} and \ref{fig:raw_tern} supports this claim.

In conclusion, this paper provided several insights that have implication for the performance of the LDA considering emoji characters. Considering that emojis are widely used on various SNS platforms and these SNS data are becoming importat as marketing and social studies, our reults suggest that emoji characters should be kept as unicode characters throughout the LDA analysis. 