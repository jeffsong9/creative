---
title: "Creative Component"
author: "Taikgun Song"
header-includes:
   - \usepackage{amsmath}
   - \usepackage{graphicx}
   - \usepackage{bbm}
   - \usepackage{subfig}
abstract: |
  Write abstract.
  It is known that the performance of Laten Dirichlet Allocation based topic models over short texts.
  In this paper, we would like to compare LDA methods with different 'parameter' under experimental setting.

output:
  pdf_document:
    fig_caption: yes
    number_sections: yes
geometry: margin=0.5in
bibliography: refs.bib
---
```{r hidecode, echo=FALSE}
library(knitr)
opts_chunk$set(echo=FALSE, warning=FALSE, message = FALSE, tidy.opts=list(width.cutoff=40))
```

```{r libraries, message=FALSE, warning=FALSE, error=FALSE}
# Load Libraries
require(openNLP)
require(NLP)
require(topicmodels)
require(tm)
require(MultinomialCI)
library(RTextTools)
library(SnowballC)
library(rvest)
library(stringr)
library(XML)
library(RCurl)
#library(gsubfn)
library(xtable)
```

```{r load data, message=FALSE, warning=FALSE, error=FALSE}
setwd("C:/Users/Taikgun Song/Desktop/CC Meeting")
dat=read.csv("data/first_ten_honolulu_restaurant.csv")
review=dat$review.entry
review=review[which(is.na(review)==F)]
```


#Introduction
Then the following `R` packages were utilized to conduct `Latent Dirichlet Allocation`[@blei2003latent] method: `openNLP`[@openNLP], `NLP` [@NLP], `topicmodels` [@topicmodels].

\pagebreak

# Data Gathering (Need revising)
In order to compare different LDA processes, a collection of short user-generated online reviews from a popular website, the Trip Adviser, was used for evaluation. Among all restaurants in Honolulu, Hawaii registered on the Trip Adviser, the latest 10 reviews were scraped and read into `R` [@R] using the `rvest` [@rvest] package. This dataset includes the following information of the 7700 Honolulu restaurants: restaurant name, number total reviews, average star rating, individual review title, individual review entry, individual star rating, and the date visited. In this paper, we are particularly interested in individual review entry.

##Procedure 1: Obtaining the page numbers for all restaurants
The following URL: "https://www.tripadvisor.com/Restaurants-g60982-Honolulu_Oahu_Hawaii.html" in Figure \ref{fig1} leads to all 1745 restaurants registered on TripAdvisor (As of October 27th, 2016).  
\begin{figure}[htbp]
\centering
\includegraphics[width=100mm]{"initial_url_image".png}
\caption{Example image of the initial TripAdvisor webpage for all restaurants in Honolulu, Hawaii \label{fig1}}
\end{figure}

\begin{figure}[htbp]
\centering
\includegraphics[width=100mm]{"page_number".png}
\caption{Example pages number image of the restaurants in Honolulu, Hawaii \label{fig2}}
\end{figure}

These restaurants are listed into 59 different pages as it is shown in Figure \ref{fig2}. Therefore, obtaining URL of all 59 pages would be the first step to scrape each individual restaurant reviews. Note that there are 30 restaurants in each page except for the last page. The following image of Figure \ref{fig3} portraits how the html code is written for this web page.

\begin{figure}[htbp]
\centering
\includegraphics[width=100mm]{"page_number_source1".png}
\caption{Image of the source code \label{fig3}}
\end{figure}

\pagebreak

###Step 1: Read html using the 'rvest' package
The following steps R code using the `rvest` [@rvest] package will scrape 59 URL address for all restaurants in Honolulu, Hawaii.
```{r, eval=FALSE, echo=T}
initial.url="https://www.tripadvisor.com/Restaurants-g60982-Honolulu_Oahu_Hawaii.html"
doc=read_html(initial.url)
```

###Step 2: Scrape the last page number of the review pages.
Note that, from Figure \ref{fig3}, the page numbers are under the node `//div [\@class="pageNumbers"]`.
Use the `html_nodes` function in conjunction with specific node using the `xpath` in the `revst` and `XML` package respectively to obtain the information under the specific node.
Then, apply the `html_text` function to obtain text information. 
Lastly, since the page numbers are in between `\\n` patterns, employ gsub function to retrieve the last page number: 59. 
```{r, eval=FALSE, echo=T}
pg.num.path=c(pg.url = '//div [@class="pageNumbers"]')
check=lapply(pg.num.path, function(x) html_nodes(doc,xpath=x))
f.page=check$pg.url %>% html_text() %>% gsub(".*\n(.*)\n", "\\1", .)
```

###Step 3: Generate urls for each review page.
The general URL for these pages are as the following: "https://www.tripadvisor.com/Restaurants-g60982-oa \underline{SUB} -Honolulu_Oahu_Hawaii.html#EATERY_LIST_CONTENTS".
There are 30 restaurants in each page, and the $1^{st}$ restaurant that appears in $n^{th}$ page is actually the $(30*(n-1)+1)^{th}$ restaurant. Therefore, changing the \underline{SUB} in the general URL with gsub function will allow us to create a list of all page links.
```{r, eval=FALSE, echo=T}
complete.initial.url=rbind.data.frame(rest.url=initial.url, data.frame(rest.url=sapply(c(1:(as.numeric(f.page)-1))*30, function(x) gsub("SUB", x,"https://www.tripadvisor.com/Restaurants-g60982-oaSUB-Honolulu_Oahu_Hawaii.html#EATERY_LIST_CONTENTS")), stringsAsFactors = F), stringsAsFactors = F)
row.names(complete.initial.url)=NULL
```

###Step 4: Combine the all the above functions
The previous R codes were combined into a single function named `get.pg.num`.
```{r, eval=FALSE, echo=T}
get.pg.num=function(intial.url){
  doc1=read_html(initial.url)
  pg.num.path=c(pg.url = '//div [@class="pageNumbers"]')
  check=lapply(pg.num.path, function(x) html_nodes(doc1,xpath=x))
  f.page=check$pg.url %>% html_text() %>% gsub(".*\n(.*)\n", "\\1", .)
  complete.initial.url=rbind.data.frame(rest.url=initial.url, data.frame(rest.url=sapply(c(1:(as.numeric(f.page)-1))*30, function(x) gsub("SUB", x,"https://www.tripadvisor.com/Restaurants-g60982-oaSUB-Honolulu_Oahu_Hawaii.html#EATERY_LIST_CONTENTS")), stringsAsFactors = F), stringsAsFactors = F)
  row.names(complete.initial.url)=NULL
  return(complete.initial.url[[1]])
}
```


##Procedure 2: Obtaining url for individual restaurants.
Once the URL for each pages are retrieved, the next step is to get URL for all restaurants within each pages.
As shown in Figure \ref{fig4}, the title of the restaurant is embedded in the html node `//div [\@id="EATERY_SEARCH_RESULTS"]//h3 [\@class="title"]` and its corresponding URL is embedded in `//div [\@id="EATERY_SEARCH_RESULTS"]//h3 [\@class="title"] //a [\@class="property_title"]`.

\begin{figure}[htbp]
\centering
\includegraphics[width=100mm]{"each_rest_url".png}
\caption{Example source code image of each restaurant URL \label{fig4}}
\end{figure}

```{r, eval=FALSE, echo=T}
doc=read_html(pg.num.url)
rest.path=c(ea.url = '//div [@id="EATERY_SEARCH_RESULTS"]//h3 [@class="title"] //a [@class="property_title"]',ea.rest.name= '//div [@id="EATERY_SEARCH_RESULTS"]//h3 [@class="title"]')
apply_rest.path=lapply(rest.path, function(x) html_nodes(doc,xpath=x))
```
Thus, the title of a restaurant and its URL is obtained by running the above R code.
Simple cleaning Code below would extract the title and the URL of each restaurant.
```{r, eval=FALSE, echo=T}
r_url.r_name=cbind.data.frame(rest.url=gsub(".*?href=\"(.*?)\".*", "https://www.tripadvisor.com\\1",apply_rest.path[[1]]), rest.name=apply_rest.path[[2]] %>% html_text() %>% gsub("\n+([^\n]+?)\n.*", "\\1", .), stringsAsFactors=F)
```
As a result, the above two codes were made into a function `rest.url`
```{r, eval=FALSE, echo=T}
rest.url=function(pg.num.url){
  doc=read_html(pg.num.url)
  rest.path=c(ea.url = '//div [@id="EATERY_SEARCH_RESULTS"]//h3 [@class="title"] //a [@class="property_title"]',ea.rest.name= '//div [@id="EATERY_SEARCH_RESULTS"]//h3 [@class="title"]')
  apply_rest.path=lapply(rest.path, function(x) html_nodes(doc,xpath=x))
  r_url.r_name=cbind.data.frame(rest.url=gsub(".*?href=\"(.*?)\".*", "https://www.tripadvisor.com\\1",apply_rest.path[[1]]), rest.name=apply_rest.path[[2]] %>% html_text() %>% gsub("\n+([^\n]+?)\n.*", "\\1", .), stringsAsFactors=F)
  return(r_url.r_name)
}
```

##Procedure 3: Obtaining individual review information for all restaurants.
After the second procedure, URL for each restaurants in Honolulu Hawaii that is registered in TripAdvisor was scraped.
These URL provide access to a list of individual review entries of that particular given restaurant. For long individual reviews, however, full entry is not directly available from the URL generated in Procedure 2.  For example, individual review illustrated in Figure \ref{fig5}, ellipsis symbol $\dots$ was used to omit reviews after `Black Sand`. Therefore, further scraping is required to retrieve full review entry. The following steps were taken.

\begin{figure}[htbp]
\centering
\includegraphics[width=100mm]{"each_review_url".png}
\caption{Example source code image of individual review URL \label{fig5}}
\end{figure}

###Step 1: Reading a given restaurant url.
As the default behavior, the `read_html` function does not properly identify itself to the server that it is retrieving the contents from. Hence, for a long process such as Procedure 3, the `useragent` should be used to the `handle` argument of `read_html` function using `curl` in order for the scraper to identify itself. Otherwise, an error message such as the following will be generated after some iteration: `Error in open.connection(x, "rb")`

```{r, eval=FALSE, echo=T}
doc=curl(ea.rest.url, handle = new_handle("useragent" = "Mozilla/5.0")) %>%  read_html()
```

###Step 2: Scraping the title of an individual review and its url.
Figure \ref{fig5} illustrates that the title of the restaurant is embedded in the html node `//div [\@id="REVIEWS"]//div [\@class="innerBubble"]//span [\@class="noQuotes"]` and its corresponding URL is embedded in `//div [\@id="REVIEWS"]//div [\@class="innerBubble"]`. The code below will generate URL for individual reviews after further cleaning with `gsub` function.
```{r, eval=FALSE, echo=T}
u.n.t.path=c(ea.url = '//div [@id="REVIEWS"]//div [@class="innerBubble"]',ea.title= '//div [@id="REVIEWS"]//div [@class="innerBubble"]//span [@class="noQuotes"]')
apply_u.n.t.path=lapply(u.n.t.path, function(x) html_nodes(doc,xpath=x))
u.n.t=cbind.data.frame(ea.url=gsub(".*?<a href=\"(.*?)\".*", "https://www.tripadvisor.com\\1",apply_u.n.t.path[[1]]), ea.title=apply_u.n.t.path[[2]] %>% html_text(), stringsAsFactors=F)
```

###Step 3: Scraping the review entry of an individual review.
\begin{figure}[htbp]
\centering
\includegraphics[width=100mm]{"each_review_url2".png}
\caption{Example source code image of individual review URL \label{fig6}}
\end{figure}

The source code and its output is presented in Figure \ref{fig6}. It is clear that the review entry is embedded in the html node `(//div [@id="REVIEWS"]//div [@class="innerBubble"]//p)[1]`. Thus, the code below was used to capture the full review entry.

```{r, eval=FALSE, echo=T}
r.path <- c(ea.review = '(//div [@id="REVIEWS"]//div [@class="innerBubble"]//p)[1]')
apply_r.path=lapply(r.path, function(x) sapply(u.n.t[,1], function(y) html_nodes(y %>% read_html(),xpath=x) %>% html_text())) %>% data.frame(stringsAsFactors=F)
u.t.r.out.put=cbind.data.frame(u.n.t, apply_r.path, stringsAsFactors=F)
```  

###Step 4: Dealing with restaurants with no review.
There are multiple restaurants without any review. These restaurants will generate error messages and should be dealt by using a simple if loop. If there is no review entry, then the `u.t.r.out.put` from the previous code will have zero number of rows. Therefore, re-define `u.t.r.out.put` with all column entry having `No review posted` with the number of rows equal to zero as the code provided below.

```{r, eval=FALSE, echo=T}
if (nrow(u.t.r.out.put)!=0){
  row.names(u.t.r.out.put)=c(1:nrow(u.n.t))
} else {u.t.r.out.put=cbind.data.frame(ea.url="No review posted", ea.title="No review posted", ea.review ="No review posted", stringsAsFactors=F)}
```

###Step 5: Combine all steps in procedure 3.
The R codes in Procedure 3 were combined into a single function named `review.data`. The function `Sys.sleep(2)` allow computer to rest for 2 seconds. Some unstable servers crash when the number of requests exceed the server's capability.  Therefore, `Sys.sleep` was introduced to avoid crashing the server.
```{r, eval=FALSE, echo=T}
review.data=function(ea.rest.url){
  doc=curl(ea.rest.url, handle = new_handle("useragent" = "Mozilla/5.0")) %>%  read_html()
  u.n.t.path=c(ea.url = '//div [@id="REVIEWS"]//div [@class="innerBubble"]',ea.title= '//div [@id="REVIEWS"]//div [@class="innerBubble"]//span [@class="noQuotes"]')
  apply_u.n.t.path=lapply(u.n.t.path, function(x) html_nodes(doc,xpath=x))
  u.n.t=cbind.data.frame(ea.url=gsub(".*?<a href=\"(.*?)\".*", "https://www.tripadvisor.com\\1",apply_u.n.t.path[[1]]), ea.title=apply_u.n.t.path[[2]] %>% html_text(), stringsAsFactors=F)
  r.path <- c(ea.review = '(//div [@id="REVIEWS"]//div [@class="innerBubble"]//p)[1]')
  apply_r.path=lapply(r.path, function(x) sapply(u.n.t[,1], function(y) html_nodes(y %>% read_html(),xpath=x) %>% html_text())) %>% data.frame(stringsAsFactors=F)
  u.t.r.out.put=cbind.data.frame(u.n.t, apply_r.path, stringsAsFactors=F)
  if (nrow(u.t.r.out.put)!=0){
    row.names(u.t.r.out.put)=c(1:nrow(u.n.t))
  } else {u.t.r.out.put=cbind.data.frame(ea.url="No review posted", ea.title="No review posted", ea.review ="No review posted", stringsAsFactors=F)}
  return(u.t.r.out.put)
  Sys.sleep(2)
}
```


##Procedure 4: Combining the previous procedures to construct full dataset.
```{r, eval=FALSE, echo=T}
# Procedure 1
initial.url="https://www.tripadvisor.com/Restaurants-g60982-Honolulu_Oahu_Hawaii.html"
source("Code/pg_num_url.r")
all.url=get.pg.num(intial.url)

# Procedure 2
source("Code/ind_rest_url.r")
ea.url=lapply(all.url, function(x) rest.url(x)) %>% do.call("rbind",.)

# Procedure 3
source("Code/ind_url_title_review.r")
fin=nrow(ea.url)

# Final Product
source("Code/review_data_w_pb.r")
final.data=lapply(1:nrow(ea.url), function(x) review.data.w.pb(x) %>% cbind(rep(ea.url[x,2], nrow(.)), .)) %>% do.call("rbind",.)
colnames(final.data)[1]="rest.name"
#View(final.data)
```
The above code combines all the previous procedures and generates the complete dataset of all restaurants in Honolulu, Hawaii with following information: Restaurant Name, URL, title, and entry of individual reviews. The combined procedure takes time, thus simple progression bar was introduced to monitor the progress. The code for the progression bar is shown below.

```{r, eval=FALSE, echo=T}
pb <- winProgressBar(title = "progress bar", min = 0, max = fin, width = 300)
review.data.w.pb=function(x){
  setWinProgressBar(pb, x, title=paste(round(x/fin*100, 0), "% done"))
  return(review.data(ea.url[x,1]))
}
```

\begin{figure}[htbp]
\centering
\includegraphics[width=150mm]{"result_data_set".png}
\caption{First 10 and the last 10 entries of the final dataset \label{fig7}}
\end{figure}

The head and the tail of the constructed dataset is given in Figure \ref{fig7}.

\pagebreak

##Procedure 5: Clean data entries for further process

The raw data scraped from the web are noisy and preparation process is necessary to minimize this noise. The initial step for this cleaning process is to remove all non-Latin characters, and any other unnecessary words or characters present in the data set collected in Procedure 4.

###Step 1: Removing reviews with non-latin characters
\begin{figure}[htbp]
\centering
\includegraphics[width=100mm]{"non-alphabet reviews".png}
\caption{Example of reviews written in non-Latin characters \label{fig8}}
\end{figure}
TripAdvisor is a globally well known website and there are reviews that are written in non-Latin characters as shown in Figure \ref{fig8}. These non-Latin characters could be deleted by the following code.

```{r, eval=F}
Encoding(final.data$ea.review) <- "latin1"
final.data$ea.review=iconv(final.data$ea.review, "latin1", "ASCII", sub="")
Encoding(final.data$ea.title) <- "latin1"
final.data$ea.title=iconv(final.data$ea.title, "latin1", "ASCII", sub="")
```

\begin{figure}[htbp]
\centering
\includegraphics[width=100mm]{"non-alphabet reviews deleted".png}
\caption{Example of deleting reviews written in non-Latin characters \label{fig9}}
\end{figure}

The result of deleting non-Latin characters could be found in Figure \ref{fig9}.

###Step 2: Removing other unnecessary words and symbols
Although non-Latin characters are deleted, there are remaining words and symbols remaining in the data set. Unicode such as <U+56FD>  in Figure \ref{fig9} is an example.  These words and symbols should be treated using gsub functions,

```{r, eval=F}
final.data=gsub("<.{6}>", "", final.data)
```


###Step 3: Removing restaurants with no review from the data set
As illustrated in Figure \ref{fig7}, Some of the restaurants on the TripAdvisor webpage has no reviews. Moreover, cleaning processes such as step 1 and 2 may remove the entire review entry for certain reviews blank. Therefore, these reviews should be removed from the data set by running a simple function as the following.

```{r, eval=F}
#final.data=final.data[!final.data$ea.review=="No review posted",]
final.data=final.data[which(!final.data$ea.review %in% c("No review posted", ""))
```

\begin{figure}[htbp]
\centering
\includegraphics[width=100mm]{"final result data set".png}
\caption{Head and tail part of the data set after under going the cleaning procedure \label{fig10}}
\end{figure}

As the result of Procedure 5, final data set was obtained  as illustrated in Figure \ref{fig10}.


\pagebreak

# Data Processing

Before running LDA, some of the techniques may be applied in order to increase the accuracy of the process.

##Removing Stop Words
Stop words are a group of natural language words that are essential in construction of grammatical structure of English. For example, words such as "a", "an", "the", "that", "these", "my", "his", "most", "with", and et cetera is an example of stop words. Although these words are crucial elements of English structure, however, these stop words are less important when it comes to the purpose of delivering the meanings. Also, high frequency and abundance nature of these stop words makes little value in helping text mining techniques, and thus be filtered out. In this paper, stop words were deleted by utilizing the `tm` [@tm] and the `RTextTools` [@RTextTools] packages in R using the following code.

```{r remove stop words, echo=F}
vdc=VectorSource(review)
vdc=VCorpus(vdc)
dtm <- DocumentTermMatrix(vdc,control=list(stopwords=T))
```

```{r, eval=F}
setwd("C:/Users/Taikgun Song/Desktop/CC Meeting")
w_sw=read.csv("output/with_stopwords.csv")
wo_sw=read.csv("output/with_out_stopwords.csv")

kable(w_sw, caption = "LDA output of the dataset before and after removing the stop words")
kable(wo_sw, caption = "LDA output of the dataset before and after removing the stop words")
```

```{r, eval=F}
# print(xtable(w.sw), file="w_sw.tex", floating=FALSE)
# print(xtable(head(wo_sw, n=10)), file="tb.tex", floating=FALSE)
```

##Stemming



















* LDA, documents are represent as random mixtures over latent topics, where each topic is characterized by a distribution over words. (Need paraphrasing)

LDA assumes that words are generated by topics.
That is, word distribution $$p(w|\theta, \beta)$$
$$p(w|\theta, \beta)=sum_{z}p(w|z,\beta)p(z|\theta)$$



Therefore, retrieving information by reducing the inflected words to its original word stem 


may increase the probability of the joint distribution of topic mixture leading (thus increasing the probability of a document and a corpus).

```{r, eval=F}
setwd("C:/Users/Taikgun Song/Desktop/CC Meeting")
sns=read.csv("output/stop n stem.csv")
print(xtable(head(sns, n=10)), file="sns.tex", floating=FALSE)
```

\begin{table}[ht]
\centering
\subfloat[Table LDA output before stemming]{\label{tab:tab2a}\scalebox{1}{\input{./wo_sw}}}\quad
\subfloat[Table LDA output after stemming]{\label{tab:tab2b}\scalebox{1}{\input{./sns}}}
\caption{Difference of LDA output between with and without stop words}
\label{tab:tab2}
\end{table}



# Sequential bigram
Wallach





```{r some-code, ref.label=all_labels()[-1], echo=FALSE, eval=FALSE}
```

$\pagebreak$

#References