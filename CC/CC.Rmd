---
title: "Creative Component"
author: "Taikgun Song"
header-includes:
   - \usepackage{graphicx}
   - \usepackage{bbm}
   - \usepackage{titlesec}
   - \usepackage{subfig}
abstract: |
  Write abstract.
  It is known that the performance of Laten Dirichlet Allocation based topic models over short texts.
  In this paper, we would like to compare LDA methods with different 'parameter' under experimental setting.

output:
  pdf_document:
    fig_caption: yes
    number_sections: yes
geometry: margin=1in
bibliography: refs.bib
---
```{r hidecode, echo=FALSE}
library(knitr)
opts_chunk$set(echo=FALSE, warning=FALSE, message = FALSE, tidy.opts=list(width.cutoff=40))
```

```{r libraries, message=FALSE, warning=FALSE, error=FALSE}
# Load Libraries
require(openNLP)
require(NLP)
require(topicmodels)
require(tm)
require(MultinomialCI)
library(RTextTools)
library(SnowballC)

library(xtable)
```

```{r load data, message=FALSE, warning=FALSE, error=FALSE}
setwd("C:/Users/Taikgun Song/Desktop/CC Meeting")
dat=read.csv("data/first_ten_honolulu_restaurant.csv")
review=dat$review.entry
review=review[which(is.na(review)==F)]
```


#Introduction
Then the following `R` packages were utilized to conduct `Latent Dirichlet Allocation`[@blei2003latent] method: `openNLP`[@openNLP], `NLP` [@NLP], `topicmodels` [@topicmodels].




# Data Cleaning
**Datasets.**
In order to compare different LDA processes, a collection of short user-generated online reviews from a popular website, the Trip Advisor, was used for evaluation. Among all restaurants in Honolulu, Hawaii registered on the Trip Advisor, the latest 10 reviews were scrapped and read into `R` [@R] using the `RCurl` [@RCurl] package. This dataset includes the following information of the 7700 Honolulu restaurants: restaurant name, number total reviews, average star rating, individual review title, individual review entry, individual star rating, and the date visited. In this paper, we are particularly interested in individual review entry. The raw data scrapped from the web are noisy and preperation process is necessary to minimize this noise. The initial step for this cleaning process is to remove all non-latin characters, and change all characters to lower case letters.

```{r remove non-latin, message=FALSE, warning=FALSE, error=FALSE}
review=tolower(review)
Encoding(review) <- "latin1"
review=iconv(review, "latin1", "ASCII", sub="")
```

**Removing Stop Words**
The second step is to remove meaningless words and words with low frequency by utilizing the `tm` [@tm] and the `RTextTools` [@RTextTools] packages in R. The listed results in Table 1 shows the importance of removing stop words prior to running LDA method.

```{r remove stop words, message=FALSE, warning=FALSE, error=FALSE}
# vdc=VectorSource(review)
# vdc=VCorpus(vdc)
# dtm <- DocumentTermMatrix(vdc,control=list(stopwords=T))
# lda=LDA(dtm,control = list(alpha = 0.1), k = 4)
# lda.out=terms(lda,10)
```

```{r, eval=F}
setwd("C:/Users/Taikgun Song/Desktop/CC Meeting")
w_sw=read.csv("output/with_stopwords.csv")
wo_sw=read.csv("output/with_out_stopwords.csv")

kable(w_sw, caption = "LDA output of the dataset before and after removing the stop words")
kable(wo_sw, caption = "LDA output of the dataset before and after removing the stop words")
```

```{r, eval=F}
# print(xtable(w.sw), file="w_sw.tex", floating=FALSE)
# print(xtable(head(wo_sw, n=10)), file="tb.tex", floating=FALSE)
```
\begin{table}[ht]
\centering
\subfloat[Table LDA output before removing the stop words]{\label{tab:tab1a}\scalebox{1}{\input{./w_sw}}}\quad
\subfloat[Table LDA output after removing the stop words]{\label{tab:tab1b}\scalebox{1}{\input{./wo_sw}}}
\caption{Difference of LDA output between with and without stop words}
\label{tab:tab1}
\end{table}

**Stemming.**

* LDA, documents are repsented as random mixtures over latent topics, where each topic is characerized by a distirbution over words. (Need paraphrasing)

LDA assumse that words are generated by topics.
That is, word distribution $$p(w|\{theta}, \{beta})$$
$$p(w|\{theta}, \{beta})=sum_{z}p(w|z,\{beta})p(z|\{theta})$$



Therefore, retrieving information by reducing the inflected words to its original word stem 


may increase the probability of the joint distribution of topic mixture leading (thus increasing the probability of a document and a corpus).

```{r, eval=F}
setwd("C:/Users/Taikgun Song/Desktop/CC Meeting")
sns=read.csv("output/stop n stem.csv")
print(xtable(head(sns, n=10)), file="sns.tex", floating=FALSE)
```

\begin{table}[ht]
\centering
\subfloat[Table LDA output before stemming]{\label{tab:tab2a}\scalebox{1}{\input{./wo_sw}}}\quad
\subfloat[Table LDA output after stemming]{\label{tab:tab2b}\scalebox{1}{\input{./sns}}}
\caption{Difference of LDA output between with and without stop words}
\label{tab:tab2}
\end{table}



# Sequential bigram
Wallach





```{r some-code, ref.label=all_labels()[-1], echo=FALSE, eval=FALSE}
```

$\pagebreak$

#References